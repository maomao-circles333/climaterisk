{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dd7b574",
   "metadata": {},
   "source": [
    "# La represa de Bhakra Nangal y el embalse de Gobind Sagar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf37081",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "La [represa de Bhakra Nangal](https://en.wikipedia.org/wiki/Bhakra_Dam) se inauguró en 1963 en la India. La represa forma el embalse de Gobind Sagar y proporciona riego a 10 millones de acres en los estados vecinos de Punjab, Haryana y Rajastán. Podemos utilizar los datos del producto OPERA DSWx para observar las fluctuaciones en el nivel del agua durante largos periodos.\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Bhakra_Dam_Aug_15_2008.JPG/440px-Bhakra_Dam_Aug_15_2008.JPG\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d981ca",
   "metadata": {},
   "source": [
    "## Esquema de las etapas del análisis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd45158c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "- Identificar los parámetros de búsqueda\n",
    "  - Área de interés (AOI) y ventana temporal\n",
    "  - _Endpoint_, proveedor, identificador del catálogo (\"nombre corto\")\n",
    "- Obtención de los resultados de la búsqueda\n",
    "  - Instrospección, análisis para identificar características, bandas de interés\n",
    "  - Almacenar los resultados en un DataFrame para facilitar la exploración\n",
    "- Exploración y refinamiento de los resultados de la búsqueda\n",
    "  - Identificar los gránulos de mayor valor\n",
    "  - Filtrar los gránulos atípicos con mínima contribución\n",
    "  - Combinar los gránulos filtrados relevantes en un DataFrame\n",
    "  - Identificar el tipo de salida a generar\n",
    "- Procesar los datos para obtener resultados relevantes\n",
    "  - Descargar los gránulos relevantes en un Xarray DataArray, apilados adecuadamente\n",
    "  - Realizar los cálculos intermedios necesarios\n",
    "  - Combinar los fragmentos de datos relevantes en la visualización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f5b6b3",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b3bb4a",
   "metadata": {},
   "source": [
    "### Importación preliminar de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a94c7ae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "import numpy as np, pandas as pd, xarray as xr\n",
    "import rioxarray as rio\n",
    "import rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124a3a8a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Imports for plotting\n",
    "import hvplot.pandas, hvplot.xarray\n",
    "import geoviews as gv\n",
    "from geoviews import opts\n",
    "gv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d14b6e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# STAC imports to retrieve cloud data\n",
    "from pystac_client import Client\n",
    "from osgeo import gdal\n",
    "# GDAL setup for accessing cloud data\n",
    "gdal.SetConfigOption('GDAL_HTTP_COOKIEFILE','~/.cookies.txt')\n",
    "gdal.SetConfigOption('GDAL_HTTP_COOKIEJAR', '~/.cookies.txt')\n",
    "gdal.SetConfigOption('GDAL_DISABLE_READDIR_ON_OPEN','EMPTY_DIR')\n",
    "gdal.SetConfigOption('CPL_VSIL_CURL_ALLOWED_EXTENSIONS','TIF, TIFF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c72870",
   "metadata": {},
   "source": [
    "### Funciones prácticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c2eb37",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# simple utility to make a rectangle with given center of width dx & height dy\n",
    "def make_bbox(pt,dx,dy):\n",
    "    '''Returns bounding-box represented as tuple (x_lo, y_lo, x_hi, y_hi)\n",
    "    given inputs pt=(x, y), width & height dx & dy respectively,\n",
    "    where x_lo = x-dx/2, x_hi=x+dx/2, y_lo = y-dy/2, y_hi = y+dy/2.\n",
    "    '''\n",
    "    return tuple(coord+sgn*delta for sgn in (-1,+1) for coord,delta in zip(pt, (dx/2,dy/2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5a10a8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# simple utility to plot an AOI or bounding-box\n",
    "def plot_bbox(bbox):\n",
    "    '''Given bounding-box, returns GeoViews plot of Rectangle & Point at center\n",
    "    + bbox: bounding-box specified as (lon_min, lat_min, lon_max, lat_max)\n",
    "    Assume longitude-latitude coordinates.\n",
    "    '''\n",
    "    # These plot options are fixed but can be over-ridden\n",
    "    point_opts = opts.Points(size=12, alpha=0.25, color='blue')\n",
    "    rect_opts = opts.Rectangles(line_width=0, alpha=0.1, color='red')\n",
    "    lon_lat = (0.5*sum(bbox[::2]), 0.5*sum(bbox[1::2]))\n",
    "    return (gv.Points([lon_lat]) * gv.Rectangles([bbox])).opts(point_opts, rect_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b87ebff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# utility to extract search results into a Pandas DataFrame\n",
    "def search_to_dataframe(search_results):\n",
    "    '''Constructs Pandas DataFrame from PySTAC Earthdata search results.\n",
    "    DataFrame columns are determined from search item properties and assets.'''\n",
    "    # Extract granules into a list of searh items\n",
    "    granules = list(search_results.items())\n",
    "    assert granules, \"Error: empty list of search results\"\n",
    "    # Determine column labels from unique properties from all granules\n",
    "    properties = sorted(list({prop for g in granules for prop in g.properties.keys()}))\n",
    "    # Assemble blocks of rows from each granule\n",
    "    blocks = []\n",
    "    for g in granules:\n",
    "        # Leftmost columns determined from properties\n",
    "        left = pd.Series(index=properties)\n",
    "        for p in properties:\n",
    "            left.loc[p] = g.properties.get(p, None)\n",
    "        tile_id = g.id.split('_')[3]\n",
    "        left.loc['tile_id'] = tile_id\n",
    "        left = pd.DataFrame(left).T\n",
    "        right = []\n",
    "        for a in sorted(g.assets.keys()):\n",
    "            href = g.assets[a].href\n",
    "            # Ignore hrefs using Amazon s3 (not currently working with rasterio)\n",
    "            if href.startswith('s3://'):\n",
    "                continue\n",
    "            right.append(pd.DataFrame(data=dict(asset=a, href=href), index=[0]))\n",
    "        # Use outer join to create block from left row and right block\n",
    "        blocks.append(left.join(pd.concat(right, axis=0, ignore_index=True), how='outer'))\n",
    "    # Stack blocks into final dataframe, forward-filling as needed\n",
    "    df = pd.concat(blocks, axis=0, ignore_index=True).ffill(axis=0)\n",
    "    assert len(df), \"Empty DataFrame\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5db695",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# utility to process DataFrame of search results & return DataArray of stacked raster images\n",
    "def stack_time_slices(granule_dataframe):\n",
    "    '''This function returns a three-dimensional Xarray DataArray comprising time slices read from GeoTIFF files.\n",
    "    - Input: a DataFrame of granules (i.e., a DataFrame with a DateTimeIndex and a column 'href' of URIs).\n",
    "    - Output: a stacked DataArray with dimensions ('time', 'longitude', 'latitude')\n",
    "    - GeoTIFF data are assumed to have been acquired over the same MGRS tile (NOT verified within).\n",
    "    - Note CRS explicitly embedded into DataArray stack as extracted from GeoTIFF file.\n",
    "    - DataArray is constructed using np.datetime64 time axis to simplify visualization.'''\n",
    "    slices, timestamps = list(), list()\n",
    "    for timestamp_, row_ in granule_dataframe.iterrows():\n",
    "        da_ = rio.open_rasterio(row_['href'])\n",
    "        # Preserve coordinate arrays from last GeoTIFF file parsed\n",
    "        x, y = da_.coords['x'].values, da_.coords['y'].values\n",
    "        slices.append(da_.values)\n",
    "        timestamps.append(np.datetime64(timestamp_,'s'))\n",
    "    # Construct time axis from accumulated timestamps\n",
    "    time = np.array(timestamps)\n",
    "    # Construct DataArray stack from accumulated slices & coordinates\n",
    "    slices = np.concatenate(slices, axis=0)\n",
    "    coords = dict(time=time, longitude=x, latitude=y)\n",
    "    stack = xr.DataArray(data=slices, coords=coords, dims=['time', 'latitude', 'longitude'])\n",
    "    # Preserve coordinate reference system (CRS) in DataArray stack\n",
    "    crs = da_.rio.crs\n",
    "    stack.rio.write_crs(crs, inplace=True)\n",
    "    return stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f2af31",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# utility to remap pixel values to a sequence of contiguous integers\n",
    "def relabel_pixels(data, values, null_val=255, transparent_val=0, replace_null=True, start=0):\n",
    "    \"\"\"\n",
    "    This function accepts a DataArray with a finite number of categorical values as entries.\n",
    "    It reassigns the pixel labels to a sequence of consecutive integers starting from start.\n",
    "    data:            Xarray DataArray with finitely many categories in its array of values.\n",
    "    null_val:        (default 255) Pixel value used to flag missing data and/or exceptions.\n",
    "    transparent_val: (default 0) Pixel value that will be fully transparent when rendered.\n",
    "    replace_null:    (default True) Maps null_value->transparent_value everywhere in data.\n",
    "    start:           (default 0) starting range of consecutive integer values for new labels.\n",
    "    The values returned are:\n",
    "    new_data:        Xarray DataArray containing pixels with new values\n",
    "    relabel:         dictionary associating old pixel values with new pixel values\n",
    "    \"\"\"\n",
    "    new_data = data.copy(deep=True)\n",
    "    if values:\n",
    "        values = np.sort(np.array(values, dtype=np.uint8))\n",
    "    else:\n",
    "        values = np.sort(np.unique(data.values.flatten()))\n",
    "    if replace_null:\n",
    "        new_data = new_data.where(new_data!=null_val, other=transparent_val)\n",
    "        values = values[np.where(values!=null_val)]\n",
    "    n_values = len(values)\n",
    "    new_values = np.arange(start=start, stop=start+n_values, dtype=values.dtype)\n",
    "    assert transparent_val in new_values, f\"{transparent_val=} not in {new_values}\"\n",
    "    relabel = dict(zip(values, new_values))\n",
    "    for old, new in relabel.items():\n",
    "        if new==old: continue\n",
    "        new_data = new_data.where(new_data!=old, other=new)\n",
    "    return new_data, relabel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad749224",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Estas funciones podrían incluirse en archivos módular para proyectos de investigación más evolucionados. Para fines didácticos, se incluyen en este cuaderno computacional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9699eafc",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e60457",
   "metadata": {},
   "source": [
    "## Identificación de los parámetros de búsqueda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71b8655",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Para las coordenadas de la represa, utilizaremos $(76.46^{\\circ}, 31.42^{\\circ})$. También buscaremos los datos de todo un año completo entre el 1 de abril de 2023 y el 1 de abril de 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8531c4ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "AOI = make_bbox((76.46, 31.42), 0.2, 0.2)\n",
    "DATE_RANGE = \"2023-04-01/2024-04-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7576bcfc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Optionally plot the AOI\n",
    "basemap = gv.tile_sources.OSM(alpha=0.5, padding=0.1)\n",
    "plot_bbox(AOI) * basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdc12c8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "search_params = dict(bbox=AOI, datetime=DATE_RANGE)\n",
    "print(search_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d28279",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fd1155",
   "metadata": {},
   "source": [
    "## Obtención de los resultados de búsqueda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a7295c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Buscaremos productos de datos OPERA DSWx, así que definimos el `ENDPOINT`, el `PROVIDER` y las `COLLECTIONS` de la siguiente manera (estos valores se modifican ocasionalmente, así que puede ser necesario hacer algunas búsquedas en el [sitio web Earthdata Search](https://search.earthdata.nasa.gov) de la NASA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474ec4b2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ENDPOINT = 'https://cmr.earthdata.nasa.gov/stac'\n",
    "PROVIDER = \"POCLOUD\"\n",
    "COLLECTIONS = [\"OPERA_L3_DSWX-HLS_V1_1.0\"]\n",
    "# Update the dictionary opts with list of collections to search\n",
    "search_params.update(collections=COLLECTIONS)\n",
    "print(search_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfaefe1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "catalog = Client.open(f'{ENDPOINT}/{PROVIDER}/')\n",
    "search_results = catalog.search(**search_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0c3cf9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Una vez que ejecutamos la búsqueda, los resultados se pueden consultar en un `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6951aad8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df = search_to_dataframe(search_results)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68d0318",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Limpiaremos el `DataFrame` `df` cambiando el nombre de la columna `eo:cloud_cover`, eliminando las columnas adicionales de fecha y hora, convirtiendo los tipos de datos de forma adecuada y seteando el índice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab462b6f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = df.rename(columns={'eo:cloud_cover':'cloud_cover'})\n",
    "df.cloud_cover = df.cloud_cover.astype(np.float16)\n",
    "df = df.drop(['start_datetime', 'end_datetime'], axis=1)\n",
    "df = df.convert_dtypes()\n",
    "df.datetime = pd.DatetimeIndex(df.datetime)\n",
    "df = df.set_index('datetime').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d367d483",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad33ec90",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "En esta fase, el `DataFrame` de los resultados de la búsqueda tendrá más de dos mil filas. Entonces, vamos a reducirlo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd8b59b",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cc9c72",
   "metadata": {},
   "source": [
    "## Exploración y refinamiento de los resultados de la búsqueda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48db552",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Filtraremos las filas del `df` para capturar solo los gránulos capturados que tengan menos del 10% de nubosidad y la banda `B01_WTR` de los datos DSWx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7570a0d6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "c1 = df.cloud_cover<10\n",
    "c2 = df.asset.str.contains('B01_WTR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9ecaf8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = df.loc[c1 & c2]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f393496",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Podemos contar todas las entradas distintas de la columna `tile_id` y encontrar que solo hay una (`T43RFQ`). Eso significa que el AOI especificado se encuentra estrictamente dentro de un mosaico MGRS único y que todos los gránulos encontrados estarán asociados a ese mosaico geográfico específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2234fd95",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df.tile_id.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cec1fd3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Redujimos el número total de gránulos a un poco más de cincuenta. Y los utilizaremos para generar una visualización."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987236bf",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a8f2d1",
   "metadata": {},
   "source": [
    "## Procesamiento los datos para obtener resultados relevantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7ee84c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Como ya vimos varias veces, apilaremos los arreglos bidimensionales de los archivos GeoTIFF listados en `df.href` en un `DataArray` tridimensional. Utilizaremos el identificador `stack` para etiquetar el resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c051adad",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "stack = stack_time_slices(df)\n",
    "stack.attrs = dict(description=f\"OPERA DSWx: B01_WTR\", units=None)\n",
    "stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365ef81b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Podemos ver los valores de los pixeles que realmente aparecen en el arreglo `stack` utilizando la función NumPy `unique`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78ab228",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "np.unique(stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ac447f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Como recordatorio, de acuerdo con la [especificación del producto DSWx](https://d2pn8kiwq2w21t.cloudfront.net/documents/ProductSpec_DSWX_URS309746.pdf), los significados de los valores ráster son los siguientes:\n",
    "\n",
    "- **0**: Sin agua&mdash;cualquier área con datos de reflectancia válidos que no sean de una de las otras categorías permitidas (agua abierta, agua superficial parcial, nieve/hielo, nube/sombra de nube, u océano enmascarado).\n",
    "- **1**: Agua abierta&mdash;cualquier píxel que sea completamente agua sin obstrucciones para el sensor, incluyendo obstrucciones por vegetación, terreno y edificios.\n",
    "- **2**: Agua parcialmente superficial&mdash;un área que es por lo menos 50% y menos de 100% agua abierta (por ejemplo, sumideros inundados, vegetación flotante, y píxeles bisecados por líneas costeras).\n",
    "- **252**: Nieve/Hielo.\n",
    "- **253**: Nube o sombra de nube&mdash;un área oscurecida por, o adyacente a, una nube o sombra de nube.\n",
    "- **254**: Océano enmascarado&mdash;un área identificada como océano utilizando una base de datos de la línea costera con un margen añadido.\n",
    "- **255**: Valor de relleno (datos faltantes).\n",
    "\n",
    "Observa que el valor `254`&mdash;océano enmascarado&mdash; no aparece en esta colección particular de rásteres porque esta región en particular está lejos de la costa.\n",
    "\n",
    "Para limpiar los datos (en caso de que querramos utilizar un mapa de colores), reasignemos los valores de los píxeles con nuestra función `relabel_pixels`. Esta vez, vamos a mantener los valores \"sin datos\" (`255`) para que podamos ver dónde faltan datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d561f2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "stack, relabel = relabel_pixels(stack, values=[0,1,2,252,253,255], replace_null=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58de9b6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Podemos ejecutar `np.unique` de nuevo para asegurarnos de que los datos se modificaron como queríamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395b8ae8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "np.unique(stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac0afa4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Ahora asignemos un mapa de colores para ayudar a visualizar las imágenes ráster. En este caso, el mapa de colores utiliza varios colores distintos con opacidad total y píxeles negros parcialmente transparentes para indicar los datos que faltan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4342bf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define a colormap using RGBA values; these need to be written manually here...\n",
    "COLORS = {\n",
    "0: (255, 255, 255, 0.0),  # Not Water\n",
    "1: (  0,   0, 255, 1.0),  # Open Water\n",
    "2: (180, 213, 244, 1.0),  # Partial Surface Water\n",
    "3: (  0, 255, 255, 1.0),  # Snow/Ice\n",
    "4: (175, 175, 175, 1.0),  # Cloud/Cloud Shadow\n",
    "5: (  0,   0, 0, 0.5),    # Missing\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c539b6c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Podemo, entonces, visualizar los datos.\n",
    "\n",
    "- Definimos las opciones adecuadas en los diccionarios `image_opts` y `layout_opts`.\n",
    "- Construimos un objeto `view` que consiste en cortes extraídos del `stack` por submuestreo de cada píxel `steps` (reduce los `steps` a `1` o `None` para ver los rásteres a resolución completa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971afeee",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "image_opts = dict(  \n",
    "                    x='longitude',\n",
    "                    y='latitude',\n",
    "                    cmap = list(COLORS.values()),\n",
    "                    colorbar=False,\n",
    "                    tiles = gv.tile_sources.OSM,\n",
    "                    tiles_opts=dict(padding=0.05, alpha=0.25),\n",
    "                    project=True,\n",
    "                    rasterize=True, \n",
    "                    framewise=False,\n",
    "                    widget_location='bottom',\n",
    "                 )\n",
    "\n",
    "layout_opts = dict(\n",
    "                    title = 'Bhakra Nangal Dam, India - water extent over a year',\n",
    "                    xlabel='Longitude (degrees)',\n",
    "                    ylabel='Latitude (degrees)',\n",
    "                    fontscale=1.25,\n",
    "                    frame_width=500, \n",
    "                    frame_height=500\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c132df",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "steps = 100\n",
    "subset = slice(0,None,steps)\n",
    "view = stack.isel(longitude=subset, latitude=subset)\n",
    "view.hvplot.image(**image_opts, **layout_opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5e4c2d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "La visualización anterior puede tardar un poco en actualizarse (según la elección de `steps`). Esta permite ver la acumulación de agua a lo largo de un año. Hay algunos cortes en los que faltan muchos datos, así que debemos tener cuidado al interpretarlos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568972ed",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
