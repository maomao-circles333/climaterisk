{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ad2fe76",
   "metadata": {},
   "source": [
    "# Incendios forestales en Grecia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c8c0c8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "En este ejemplo, recuperaremos los datos asociados a los [incendios forestales en Grecia en el 2023](https://es.wikipedia.org/wiki/Incendios_forestales_en_Grecia_de_2023) para comprender su evolución y extensión. Generaremos una serie temporal asociada a estos datos y dos visualizaciones del evento.\n",
    "\n",
    "En particular, analizaremos la zona que está alrededor de la ciudad de [Alexandroupolis](https://es.wikipedia.org/wiki/Alejandr%C3%B3polis), que se vio gravemente afectada por incendios forestales, con la consiguiente pérdida de vidas, propiedades y zonas boscosas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470e5a57",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b886ae5",
   "metadata": {},
   "source": [
    "## Esquema de las etapas del análisis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bec6497",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "- Identificación de los parámetros de búsqueda\n",
    "  - AOI, ventana temporal\n",
    "  - _Endpoint_, proveedor, identificador del catálogo (\"nombre corto\")\n",
    "- Obtención de los resultados de la búsqueda\n",
    "  - Inspección, análisis para identificar características, bandas de interés\n",
    "  - Almacenamos los resultados en un DataFrame para facilitar la exploración\n",
    "- Exploración y refinamiento de los resultados de la búsqueda\n",
    "  - Identificar los gránulos de mayor valor\n",
    "  - Filtrar los gránulos extraños con una contribución mínima\n",
    "  - Integrar los gránulos filtrados en un DataFrame\n",
    "  - Identificar el tipo de resultado que se quiere generar\n",
    "- Procesamiento de los datos para generar resultados relevantes\n",
    "  - Descargar los gránulos relevantes en Xarray DataArray, apilados adecuadamente\n",
    "  - Llevar a cabo los cálculos intermedios necesarios\n",
    "  - Unir los fragmentos de datos relevantes en la visualización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caaa05c",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2abb6e",
   "metadata": {},
   "source": [
    "### Importación preliminar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0770230",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "import numpy as np, pandas as pd, xarray as xr\n",
    "import rioxarray as rio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc5b298",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Imports for plotting\n",
    "import hvplot.pandas, hvplot.xarray\n",
    "import geoviews as gv\n",
    "from geoviews import opts\n",
    "gv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d66463",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# STAC imports to retrieve cloud data\n",
    "from pystac_client import Client\n",
    "from osgeo import gdal\n",
    "# GDAL setup for accessing cloud data\n",
    "gdal.SetConfigOption('GDAL_HTTP_COOKIEFILE','~/.cookies.txt')\n",
    "gdal.SetConfigOption('GDAL_HTTP_COOKIEJAR', '~/.cookies.txt')\n",
    "gdal.SetConfigOption('GDAL_DISABLE_READDIR_ON_OPEN','EMPTY_DIR')\n",
    "gdal.SetConfigOption('CPL_VSIL_CURL_ALLOWED_EXTENSIONS','TIF, TIFF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d4858a",
   "metadata": {},
   "source": [
    "### Funciones prácticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a805d657",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# simple utility to make a rectangle with given center of width dx & height dy\n",
    "def make_bbox(pt,dx,dy):\n",
    "    '''Returns bounding-box represented as tuple (x_lo, y_lo, x_hi, y_hi)\n",
    "    given inputs pt=(x, y), width & height dx & dy respectively,\n",
    "    where x_lo = x-dx/2, x_hi=x+dx/2, y_lo = y-dy/2, y_hi = y+dy/2.\n",
    "    '''\n",
    "    return tuple(coord+sgn*delta for sgn in (-1,+1) for coord,delta in zip(pt, (dx/2,dy/2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4639f0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# simple utility to plot an AOI or bounding-box\n",
    "def plot_bbox(bbox):\n",
    "    '''Given bounding-box, returns GeoViews plot of Rectangle & Point at center\n",
    "    + bbox: bounding-box specified as (lon_min, lat_min, lon_max, lat_max)\n",
    "    Assume longitude-latitude coordinates.\n",
    "    '''\n",
    "    # These plot options are fixed but can be over-ridden\n",
    "    point_opts = opts.Points(size=12, alpha=0.25, color='blue')\n",
    "    rect_opts = opts.Rectangles(line_width=2, alpha=0.1, color='red')\n",
    "    lon_lat = (0.5*sum(bbox[::2]), 0.5*sum(bbox[1::2]))\n",
    "    return (gv.Points([lon_lat]) * gv.Rectangles([bbox])).opts(point_opts, rect_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fb664b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# utility to extract search results into a Pandas DataFrame\n",
    "def search_to_dataframe(search_results):\n",
    "    '''Constructs Pandas DataFrame from PySTAC Earthdata search results.\n",
    "    DataFrame columns are determined from search item properties and assets.'''\n",
    "    # Extract granules into a list of searh items\n",
    "    granules = list(search_results.items())\n",
    "    assert granules, \"Error: empty list of search results\"\n",
    "    # Determine column labels from unique properties from all granules\n",
    "    properties = sorted(list({prop for g in granules for prop in g.properties.keys()}))\n",
    "    # Assemble blocks of rows from each granule\n",
    "    blocks = []\n",
    "    for g in granules:\n",
    "        # Leftmost columns determined from properties\n",
    "        left = pd.Series(index=properties)\n",
    "        for p in properties:\n",
    "            left.loc[p] = g.properties.get(p, None)\n",
    "        tile_id = g.id.split('_')[3]\n",
    "        left.loc['tile_id'] = tile_id\n",
    "        left = pd.DataFrame(left).T\n",
    "        right = []\n",
    "        for a in sorted(g.assets.keys()):\n",
    "            href = g.assets[a].href\n",
    "            # Ignore hrefs using Amazon s3 (not currently working with rasterio)\n",
    "            if href.startswith('s3://'):\n",
    "                continue\n",
    "            right.append(pd.DataFrame(data=dict(asset=a, href=href), index=[0]))\n",
    "        # Use outer join to create block from left row and right block\n",
    "        blocks.append(left.join(pd.concat(right, axis=0, ignore_index=True), how='outer'))\n",
    "    # Stack blocks into final dataframe, forward-filling as needed\n",
    "    df = pd.concat(blocks, axis=0, ignore_index=True).ffill(axis=0)\n",
    "    assert len(df), \"Empty DataFrame\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90638a90",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def stack_time_slices(granule_dataframe):\n",
    "    '''This function returns a three-dimensional Xarray DataArray comprising time slices read from GeoTIFF files.\n",
    "    - Input: a DataFrame of granules (i.e., a DataFrame with a DateTimeIndex and a column 'href' of URIs).\n",
    "    - Output: a stacked DataArray with dimensions ('time', 'longitude', 'latitude')\n",
    "    - GeoTIFF data are assumed to have been acquired over the same MGRS tile (NOT verified within).\n",
    "    - Note CRS explicitly embedded into DataArray stack as extracted from GeoTIFF file.\n",
    "    - DataArray is constructed using np.datetime64 time axis to simplify visualization.'''\n",
    "    slices, timestamps = list(), list()\n",
    "    for timestamp_, row_ in granule_dataframe.iterrows():\n",
    "        da_ = rio.open_rasterio(row_['href'])\n",
    "        # Preserve coordinate arrays from last GeoTIFF file parsed\n",
    "        x, y = da_.coords['x'].values, da_.coords['y'].values\n",
    "        slices.append(da_.values)\n",
    "        timestamps.append(np.datetime64(timestamp_,'s'))\n",
    "    # Construct time axis from accumulated timestamps\n",
    "    time = np.array(timestamps)\n",
    "    # Construct DataArray stack from accumulated slices & coordinates\n",
    "    slices = np.concatenate(slices, axis=0)\n",
    "    coords = dict(time=time, longitude=x, latitude=y)\n",
    "    stack = xr.DataArray(data=slices, coords=coords, dims=['time', 'latitude', 'longitude'])\n",
    "    # Preserve coordinate reference system (CRS) in DataArray stack\n",
    "    crs = da_.rio.crs\n",
    "    stack.rio.write_crs(crs, inplace=True)\n",
    "    return stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a686355",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Estas funciones podrían incluirse en archivos modular para proyectos de investigación más evolucionados. Para fines didácticos, se incluyen en este cuaderno computacional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b47d1",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7cb44b",
   "metadata": {},
   "source": [
    "## Identificación de los parámetros de búsqueda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb9c3a3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dadia_forest = (26.18, 41.08)\n",
    "AOI = make_bbox(dadia_forest, 0.1, 0.1)\n",
    "DATE_RANGE = '2023-08-01/2023-09-30'.split('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811ef6b6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Optionally plot the AOI\n",
    "basemap = gv.tile_sources.ESRI(width=500, height=500, padding=0.1, alpha=0.25)\n",
    "plot_bbox(AOI) * basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b717601a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "search_params = dict(bbox=AOI, datetime=DATE_RANGE)\n",
    "print(search_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b558d9bf",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde5feb1",
   "metadata": {},
   "source": [
    "## Obtención de los resultados de búsqueda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3528d1f8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ENDPOINT = 'https://cmr.earthdata.nasa.gov/stac'\n",
    "PROVIDER = 'LPCLOUD'\n",
    "COLLECTIONS = [\"OPERA_L3_DIST-ALERT-HLS_V1_1\"]\n",
    "# Update the dictionary opts with list of collections to search\n",
    "search_params.update(collections=COLLECTIONS)\n",
    "print(search_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f4f61c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "catalog = Client.open(f'{ENDPOINT}/{PROVIDER}/')\n",
    "search_results = catalog.search(**search_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0545acd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Como de costumbre, codificaremos el resultado de la búsqueda en un Pandas `DataFrame`, analizaremos los resultados, y haremos algunas transformaciones para limpiarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a6af49",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df = search_to_dataframe(search_results)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bd57d7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Limpiaremos el `DataFrame` `df` de las formas típicas:\n",
    "\n",
    "- convertiendo la columna `datetime` en `DatetimeIndex`,\n",
    "- eliminando columnas de tipo `datetime` extrañas,\n",
    "- renombrando la columna `eo:cloud_cover` como `cloud_cover`,\n",
    "- convirtiendo la columna `cloud_cover` en valores de punto flotante, y\n",
    "- convertiendo las columnas restantes en cadenas de caracteres, y\n",
    "- estableciendo la columna `datetime` como `Index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9765d794",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(['end_datetime', 'start_datetime'], axis=1)\n",
    "df.datetime = pd.DatetimeIndex(df.datetime)\n",
    "df = df.rename(columns={'eo:cloud_cover':'cloud_cover'})\n",
    "df['cloud_cover'] = df['cloud_cover'].astype(np.float16)\n",
    "for col in ['asset', 'href', 'tile_id']:\n",
    "    df[col] = df[col].astype(pd.StringDtype())\n",
    "df = df.set_index('datetime').sort_index()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7dc99b",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48fdf2e",
   "metadata": {},
   "source": [
    "## Exploración y refinamiento de los resultados de la búsqueda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668aade1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Examinemos el `DataFrame` `df` para comprender mejor los resultados de la búsqueda. En primer lugar, veamos cuántos mosaicos geográficos diferentes aparecen en los resultados de la búsqueda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ff40e1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df.tile_id.value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1093c4e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Entonces, el AOI se encuentra estrictamente dentro de un único mosaico geográfico MGRS llamado `T35TMF`. Analicemos la columna `asset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a388da",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df.asset.value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97792f17",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Algunos de los nombres de estos activos no son tan simples y ordenados como los que encontramos con los productos de datos DIST-ALERT. Sin embargo, podemos identificar fácilmente las subcadenas de caracteres útiles. En este caso, elegimos sólo las filas en las que la columna `asset` incluye `'VEG-DIST-STATUS'` como subcadena de caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37efbf9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "idx_veg_dist_status = df.asset.str.contains('VEG-DIST-STATUS')\n",
    "idx_veg_dist_status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fe9181",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Podemos utilizar esta `Series` booleana con el método de acceso `.loc` de Pandas para filtrar solo las filas que queremos (por ejemplo, las que se conectan a archivos de datos ráster que almacenan la banda `VEG-DIST-STATUS`). Posteriormente podemos eliminar la columna `asset` (que ya no es necesaria)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83302089",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "veg_dist_status = df.loc[idx_veg_dist_status]\n",
    "veg_dist_status = veg_dist_status.drop('asset', axis=1)\n",
    "veg_dist_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec96b1f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(len(veg_dist_status))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0135add8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Observe que algunas de las filas tienen la misma fecha pero horas diferentes (lo cual corresponde a varias observaciones en el mismo día del calendario UTC). Podemos agregar las URL en listas _remuestreando_ la serie temporal por día. Posteriormente podremos visualizar el resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d434a2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "by_day = veg_dist_status.resample('1d').href.apply(list)\n",
    "display(by_day)\n",
    "by_day.map(len).hvplot.scatter(grid=True).opts(title='# of observations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be6be5c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Limpiemos la `Series` `by_day` filtrando las filas que tienen listas vacías (es decir, fechas en las que no se adquirieron datos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782f5214",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "by_day = by_day.loc[by_day.map(bool)]\n",
    "by_day.map(len).hvplot.scatter(ylim=(0,2.1), grid=True).opts(title=\"# of observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765b9c77",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Ahora podemos utilizar la serie `by_day` remuestreada para extraer datos ráster para su análisis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31d5001",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d41e8a9",
   "metadata": {},
   "source": [
    "## Procesamiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef3ec32",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "El incendio forestal cerca de Alexandroupolis comenzó alrededor del 21 de agosto y se propagó rápidamente, afectando en particular al cercano bosque de Dadia. En primer lugar, vamos a ensamblar un \"cubo de datos\" (por ejemplo, un arreglo apilado de rásters) a partir de los archivos remotos indexados en la serie Pandas `by_day`. Empezaremos seleccionando y cargando uno de los archivos GeoTIFF remotos para extraer los metadatos que se aplican a todos los rásteres asociados con este evento y este mosaico MGRS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4aa7cc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "href = by_day[0][0]\n",
    "data = rio.open_rasterio(href).rename(dict(x='longitude', y='latitude'))\n",
    "crs = data.rio.crs\n",
    "shape = data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f2519f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Antes de construir un `DataArray` apilado dentro de un bucle, definiremos un diccionario de Python llamado `template` que se utilizará para instanciar los cortes del arreglo. El diccionario `template` almacenará los metadatos extraídos del archivo GeoTIFF, especialmente las coordenadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d15112",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "template = dict()\n",
    "template['coords'] = data.coords.copy()\n",
    "del template['coords']['band']\n",
    "template['coords'].update({'time': by_day.index.values})\n",
    "template['dims'] = ['time', 'latitude', 'longitude']\n",
    "template['attrs'] = dict(description=f\"OPERA DSWX: VEG-DIST-STATUS\", units=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6078304",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4d04aa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Utilizaremos un bucle para construir un arreglo apilado de rásteres a partir de la serie Pandas `by_day` (cuyas entradas son listas de cadenas de caracteres, es decir, URIs). Si la lista tiene un único elemento, la URL se puede leer directamente utilizando `rasterio.open`; de lo contrario, la función [`rasterio.merge.merge`](https://rasterio.readthedocs.io/en/latest/api/rasterio.merge.html) combina varios archivos de datos ráster adquiridos el mismo día en una única imagen ráster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8047b5af",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.merge import merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3607254d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rasters = []\n",
    "for date, hrefs in by_day.items():\n",
    "    n_files = len(hrefs)\n",
    "    if n_files > 1:\n",
    "        print(f\"Merging {n_files} files for {date.strftime('%Y-%m-%d')}...\")\n",
    "        raster, _ = merge(hrefs)\n",
    "    else:\n",
    "        print(f\"Opening {n_files} file  for {date.strftime('%Y-%m-%d')}...\")\n",
    "        with rasterio.open(hrefs[0]) as ds:\n",
    "            raster = ds.read()\n",
    "    rasters.append(np.reshape(raster, newshape=shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea46995",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Los datos acumulados en la lista de `rasters` se almacenan como arreglos de NumPy. Así, en vez de llamar a `xarray.concat`, realizamos una llamada a `numpy.concatenate` dentro de una llamada al constructor `xarray.DataArray`. Vinculamos el objeto creado al identificador `stack`, asegurándonos de incluir también la información CRS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4145fbfd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "stack = xr.DataArray(data=np.concatenate(rasters, axis=0), **template)\n",
    "stack.rio.write_crs(crs, inplace=True)\n",
    "stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301f02d2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "La pila `DataArray` `stack` tiene `time`, `longitude` y `latitude` como principales dimensiones de coordenadas. Podemos utilizarla para hacer algunos cálculos y generar visualizaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9b3243",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a7592d",
   "metadata": {},
   "source": [
    "### Visualización del área dañada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48649646",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Para empezar, utilicemos los datos en `stack` para calcular la superficie total dañada. Los datos en `stack` provienen de la banda `VEG-DIST-STATUS` del producto DIST-ALERT. Interpretamos los valores de los píxeles en esta banda de la siguiente manera:\n",
    "\n",
    "- **0:** Sin alteración\n",
    "- **1:** La primera detección de alteraciones con cambio en la cobertura vegetal $<50\\%$\n",
    "- **2:** Detección provisional de alteraciones con cambio en la cobertura vegetal $<50\\%$\n",
    "- **3:** Detección confirmada de alteraciones con cambio en la cobertura vegetal $<50\\%$\n",
    "- **4:** La primera detección de alteraciones con cambio en la cobertura vegetal $\\ge50\\%$\n",
    "- **5:** Detección provisional de alteraciones con cambio en la cobertura vegetal $\\ge50\\%$\n",
    "- **6:** Detección confirmada de alteraciones con cambio en la cobertura vegetal $\\ge50\\%$\n",
    "- **7:** Detección finalizada de alteraciones con cambio en la cobertura vegetal $<50\\%$\n",
    "- **8:** Detección finalizada de alteraciones con cambio en la cobertura vegetal $\\ge50\\%$\n",
    "\n",
    "El valor del píxel particular que queremos marcar es 6, es decir, un píxel en el que se confirmó que por lo menos el 50% de la cubierta vegetal está dañada y en el que la alteración continúa activamente. Podemos utilizar el método `.sum` para sumar todos los píxeles con valor `6` y el método `.to_series` para representar la suma como una serie de Pandas indexada en el tiempo. También definimos `conversion_factor` que toma en cuenta el área de cada píxel en $\\mathrm{km}^2$ (recordemos que cada píxel tiene un área de $30\\mathrm{m}\\times30\\mathrm{m}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d5bf35",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pixel_val = 6\n",
    "conversion_factor = (30/1_000)**2 / pixel_val\n",
    "damage_area = stack.where(stack==pixel_val, other=0).sum(axis=(1,2)) \n",
    "damage_area = damage_area.to_series() * conversion_factor\n",
    "damage_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73f7a49",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plot_title = 'Damaged Area (km²)'\n",
    "line_plot_opts = dict(title=plot_title, grid=True, color='r')\n",
    "damage_area.hvplot.line(**line_plot_opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05036431",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Observando el gráfico anterior, parece que los incendios forestales comenzaron alrededor del 21 de agosto y se propagaron rápidamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512cd5a8",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d350156e",
   "metadata": {},
   "source": [
    "### Visualización de fragmentos temporales seleccionados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ca04bd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "El bosque cercano de Dadia se vio especialmente afectado por los incendios. Para comprobarlo, trazaremos los datos ráster para ver la distribución espacial de los píxeles dañados en tres fechas concretas: 2 de agosto, 26 de agosto y 18 de septiembre. Una vez más, resaltaremos solamente los píxeles que tengan valor 6 en los datos ráster. Podemos extraer fácilmente esas fechas específicas de la serie temporal `by_day` utilizando una lista de fechas (por ejemplo, `dates_of_interest` en la siguiente celda)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50f59f5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dates_of_interest = ['2023-08-01', '2023-08-26', '2023-09-18']\n",
    "print(dates_of_interest)\n",
    "snapshots = stack.sel(time=dates_of_interest)\n",
    "snapshots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca6a117",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Vamos a hacer una secuencia estática de los gráficos. Empezaremos definiendo algunas opciones estándar almacenadas en diccionarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee253fcc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "image_opts = dict(\n",
    "                    x='longitude', \n",
    "                    y='latitude',\n",
    "                    rasterize=True, \n",
    "                    dynamic=True,\n",
    "                    crs=crs,\n",
    "                    shared_axes=False,\n",
    "                    colorbar=False,\n",
    "                    aspect='equal',\n",
    "                 )\n",
    "layout_opts = dict(xlabel='Longitude', ylabel=\"Latitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c6df15",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Construiremos un mapa de colores utilizando un diccionario de valores RGBA (por ejemplo, tuplas con tres entradas enteras entre 0 y 255, y una cuarta entrada de punto flotante entre 0.0 y 1.0 para la transparencia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2e42b2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "COLORS = { k:(0,0,0,0.0) for k in range(256) }\n",
    "COLORS.update({6: (255,0,0,1.0)})\n",
    "image_opts.update(cmap=list(COLORS.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f1514b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Como siempre, empezaremos por cortar imágenes más pequeñas para asegurarnos de que `hvplot.image` funciona correctamente. Podemos reducir el valor del parámetro `steps` a `1` o `None` para obtener las imágenes renderizadas en resolución completa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494393e4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "steps = 100\n",
    "subset = slice(0,None, steps)\n",
    "view = snapshots.isel(longitude=subset, latitude=subset)\n",
    "(view.hvplot.image(**image_opts).opts(**layout_opts) * basemap).layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aada3d14",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Si eliminamos la llamada a `.layout`, podemos producir un desplazamiento interactivo que muestre el progreso del incendio forestal utilizando todos los rásteres en `stack`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb97d5e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "steps = 100\n",
    "subset = slice(0,None, steps)\n",
    "view = stack.isel(longitude=subset, latitude=subset,)\n",
    "(view.hvplot.image(**image_opts).opts(**layout_opts) * basemap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4367f4",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
