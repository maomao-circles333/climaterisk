{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6f80d8",
   "metadata": {},
   "source": [
    "# Uso de la API de PySTAC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8dbe40",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "En el sitio web [Earthdata Search](https://search.earthdata.nasa.gov) de la NASA se puede buscar una gran cantidad de datos. El enlace anterior se conecta a una interfaz gráfica de usuario (GUI, por sus siglas en inglés de _Graphical User Interface_) para buscar en los [catálogos activos espaciotemporales (STACs, por sus siglas en inglés de _SpatioTemporal Asset Catalogs_)](https://stacspec.org/) al especificar un área de interés (AOI, por sus siglas en inglés de _Area of Interest_) y una _ventana temporal_ o un _intervalo de fechas_.\n",
    "\n",
    "En pos de la reproducibilidad, se busca que las personas usuarias sean capaces de buscar en los catálogos de activos de manera programática. Aquí es donde entra en juego la librería [PySTAC](https://pystac.readthedocs.io/en/stable/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f43efa0",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1303098f",
   "metadata": {},
   "source": [
    "## Esquema de las etapas del análisis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e236272",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "- Identificación de los parámetros de búsqueda\n",
    "  - AOI, ventana temporal\n",
    "  - Endpoint, proveedor, identificador del catálogo (\"nombre corto\")\n",
    "- Obtención de los resultados de la búsqueda\n",
    "  - Exploración de datos, análisis para identificar características, bandas de interés\n",
    "  - Almacenar los resultados en un DataFrame para facilitar la exploración\n",
    "- Explorar y refinar los resultados de la búsqueda\n",
    "  - Identificar los granos de mayor valor\n",
    "  - Filtrar los granos anómalos con una contribución mínima\n",
    "  - Combinar los granos filtrados correspondientes en un DataFrame\n",
    "  - Identificar el tipo de salida que se quiere obtener\n",
    "- Procesamiento de los datos para generar resultados relevantes\n",
    "  - Descargar los granos relevantes en un tipo de dato DataArray de la libreria Xarray, apilados adecuadamente\n",
    "  - Realizar los cálculos intermedios necesarios\n",
    "  - Integrar los fragmentos de datos relevantes en una visualización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa47dc6",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2896e26b",
   "metadata": {},
   "source": [
    "## Identificar los parámetros de búsqueda\n",
    "\n",
    "### Definir el AOI y el rango de fechas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e03bc63",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Comenzaremos tomando en cuenta un ejemplo concreto. [Las fuertes lluvias afectaron gravemente al sureste de Texas en mayo de 2024](https://www.texastribune.org/2024/05/03/texas-floods-weather-harris-county/), provocando [inundaciones y causando importantes daños materiales y humanos](https://www.texastribune.org/series/east-texas-floods-2024/).\n",
    "\n",
    "Como es usual, se requiere la importación de ciertas librerías relevantes. Las dos primeras celdas son familiares (relacionadas con las herramientas de análisis y la visualización de los datos que ya se examinaron). La tercera celda incluye la importación de la biblioteca `pystac_client` y de la biblioteca `gdal`, seguidas de algunos ajustes necesarios para utilizar la [Biblioteca de Abstracción de Datos Geoespaciales (GDAL, por sus siglas en inglés, Geospatial Data Abstraction Library)](https://gdal.org). Estos detalles en la configuración permiten que las sesiones de tu cuaderno computacional interactúen sin problemas con las fuentes remotas de datos geoespaciales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2771ba9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "# data wrangling imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import rioxarray as rio\n",
    "import rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dc5a79",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Imports for plotting\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray\n",
    "import geoviews as gv\n",
    "from geoviews import opts\n",
    "gv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce8454e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# STAC imports to retrieve cloud data\n",
    "from pystac_client import Client\n",
    "from osgeo import gdal\n",
    "# GDAL setup for accessing cloud data\n",
    "gdal.SetConfigOption('GDAL_HTTP_COOKIEFILE','~/.cookies.txt')\n",
    "gdal.SetConfigOption('GDAL_HTTP_COOKIEJAR', '~/.cookies.txt')\n",
    "gdal.SetConfigOption('GDAL_DISABLE_READDIR_ON_OPEN','EMPTY_DIR')\n",
    "gdal.SetConfigOption('CPL_VSIL_CURL_ALLOWED_EXTENSIONS','TIF, TIFF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82bb724",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "A continuación, definiremos los parámetros de búsqueda geográfica para poder recuperar los datos correspondientes a ese evento de inundación. Esto consiste en especificar un _AOI_ y un _intervalo de fechas_.\n",
    "\n",
    "- El AOI se especifica como un rectángulo de coordenadas de longitud-latitud en una única 4-tupla con la forma\n",
    "  $$({\\mathtt{longitude}}_{\\mathrm{min}},{\\mathtt{latitude}}_{\\mathrm{min}},{\\mathtt{longitude}}_{\\mathrm{max}},{\\mathtt{latitude}}_{\\mathrm{max}}),$$\n",
    "  por ejemplo, las coordenadas de la esquina inferior izquierda seguidas de las coordenadas de la esquina superior derecha.\n",
    "- El intervalo de fechas se especifica como una cadena de la forma\n",
    "  $${\\mathtt{date}_{\\mathrm{start}}}/{\\mathtt{date}_{\\mathrm{end}}},$$\n",
    "  donde las fechas se especifican en el formato estándar `YYYY-MM-DD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a64be93",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Center of the AOI\n",
    "livingston_tx_lonlat = (-95.09,30.69) # (lon, lat) form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef30fed0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Escribiremos algunas funciones cortas para encapsular la lógica de nuestros flujos de trabajo genéricos. Para el código de investigación, estas se colocarían en archivos de Python. Por practicidad, incrustaremos las funciones en este cuaderno y en otros para que puedan ejecutarse correctamente con dependencias mínimas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c8f234",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# simple utility to make a rectangle with given center of width dx & height dy\n",
    "def make_bbox(pt,dx,dy):\n",
    "    '''Returns bounding-box represented as tuple (x_lo, y_lo, x_hi, y_hi)\n",
    "    given inputs pt=(x, y), width & height dx & dy respectively,\n",
    "    where x_lo = x-dx/2, x_hi=x+dx/2, y_lo = y-dy/2, y_hi = y+dy/2.\n",
    "    '''\n",
    "    return tuple(coord+sgn*delta for sgn in (-1,+1) for coord,delta in zip(pt, (dx/2,dy/2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e944da",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# simple utility to plot an AOI or bounding-box\n",
    "def plot_bbox(bbox):\n",
    "    '''Given bounding-box, returns GeoViews plot of Rectangle & Point at center\n",
    "    + bbox: bounding-box specified as (lon_min, lat_min, lon_max, lat_max)\n",
    "    Assume longitude-latitude coordinates.\n",
    "    '''\n",
    "    # These plot options are fixed but can be over-ridden\n",
    "    point_opts = opts.Points(size=12, alpha=0.25, color='blue')\n",
    "    rect_opts = opts.Rectangles(line_width=0, alpha=0.1, color='red')\n",
    "    lon_lat = (0.5*sum(bbox[::2]), 0.5*sum(bbox[1::2]))\n",
    "    return (gv.Points([lon_lat]) * gv.Rectangles([bbox])).opts(point_opts, rect_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814ec43a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "AOI = make_bbox(livingston_tx_lonlat, 0.5, 0.25)\n",
    "basemap = gv.tile_sources.OSM.opts(width=500, height=500)\n",
    "plot_bbox(AOI) * basemap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814e85af",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Agreguemos un intervalo de fechas. Las inundaciones ocurrieron principalmente entre el 30 de abril y el 2 de mayo. Estableceremos una ventana temporal más larga que cubra los meses de abril y mayo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a4f7da",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "start_date, stop_date = '2024-04-01', '2024-05-31'\n",
    "DATE_RANGE = f'{start_date}/{stop_date}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fccaf5e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Por último, se crea un un diccionario `search_params` que almacene el AOI y el intervalo de fechas. Este diccionario se utilizará para buscar datos en los STACs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169cf96c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "search_params = dict(bbox=AOI, datetime=DATE_RANGE)\n",
    "print(search_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ee04f9",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ae05d3",
   "metadata": {},
   "source": [
    "## Obtención de los resultados de búsqueda\n",
    "\n",
    "### Ejecución de una búsqueda con la API PySTAC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522b0a6a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Para iniciar una búsqueda de datos se necesitan tres datos más: el _Endpoint_ (una URL), el _Proveedor_ (una cadena que representa una ruta que extiende el _Endpoint_) y los _Identificadores de la colección_ (una lista de cadenas que hacen referencia a catálogos específicos). Generalmente, debemos probar con el [sitio web de Earthdata Search](https://search.earthdata.nasa.gov) de la NASA para determinar correctamente los valores para los productos de datos específicos que queremos recuperar. El [repositorio de GitHub de la NASA CMR STAC también supervisa los problemas](https://github.com/nasa/cmr-stac/issues) relacionados con la API para las consultas de búsqueda de Earthdata Cloud.\n",
    "\n",
    "Para la búsqueda de productos de datos DSWx que se quiere ejecutar, estos parámetros son los que se definen en la siguiente celda de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f868782e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ENDPOINT = 'https://cmr.earthdata.nasa.gov/stac' # base URL for the STAC to search\n",
    "PROVIDER = 'POCLOUD'\n",
    "COLLECTIONS = [\"OPERA_L3_DSWX-HLS_V1_1.0\"]\n",
    "# Update the dictionary opts with list of collections to search\n",
    "search_params.update(collections=COLLECTIONS)\n",
    "print(search_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff46046",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Una vez que se definieron los parámetros de búsqueda en el diccionario de Python `search_params`, se puede instanciar un `Cliente` y buscar en el catálogo espacio-temporal de activos utilizando el método `Client.search`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0d62e2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "catalog = Client.open(f'{ENDPOINT}/{PROVIDER}/')\n",
    "search_results = catalog.search(**search_params)\n",
    "print(f'{type(search_results)=}\\n',search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf483a8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "El objeto `search_results` que se obtuvo al llamar al método `search` es del tipo `ItemSearch`. Para recuperar los resultados, invocamos al método `items` y convertimos el resultado en una `list` de Python que asociaremos al identificador `granules`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16115ce1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "granules = list(search_results.items())\n",
    "print(f\"Number of granules found with tiles overlapping given AOI: {len(granules)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad4a982",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Se analiza el contenido de la lista `granules`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b902ba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "granule = granules[0]\n",
    "print(f'{type(granule)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae48b447",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "granule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc55a3d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "El objeto `granule` tiene una representación de salida enriquecida en este cuaderno computacional de Jupyter. Podemos ampliar los atributos en la celda de salida haciendo clic en los triángulos.\n",
    "\n",
    "![](../../../assets/img/granule_output_repr.png)\n",
    "\n",
    "El término _grano_ se refiere a una colección de archivos de datos (datos ráster en este caso), todos ellos asociados a datos sin procesar adquiridos por un satélite concreto en una fecha y hora fija sobre un mosaico geográfico concreto. Hay una gran variedad de atributos interesantes asociados con este grano.\n",
    "\n",
    "- properties['datetime']: una cadena que representa la hora de adquisición de los datos de los archivos de datos ráster de este grano,\n",
    "- properties['eo:cloud_cover']: el porcentaje de píxeles oscurecidos por nubes y sombras de nubes en los archivos de datos ráster de este grano, y\n",
    "- `assets`: un `dict` de Python cuyos valores resumen las bandas o niveles de los datos ráster asociados con este gránulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed82a9b1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(f\"{type(granule.properties)=}\\n\")\n",
    "print(f\"{granule.properties['datetime']=}\\n\")\n",
    "print(f\"{granule.properties['eo:cloud_cover']=}\\n\")\n",
    "print(f\"{type(granule.assets)=}\\n\")\n",
    "print(f\"{granule.assets.keys()=}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7f65aa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Cada objeto en `granule.assets` es una instancia de la clase `Asset` que tiene un atributo `href`. Es el atributo `href` el que nos indica dónde localizar un archivo GeoTiff asociado con el activo de este gránulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60e4e07",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for a in granule.assets:\n",
    "    print(f\"{a=}\\t{type(granule.assets[a])=}\")\n",
    "    print(f\"{granule.assets[a].href=}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1610a1f2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Además, el `Item` tiene un atributo `.id` que almacena una cadena de caracteres. Al igual que ocurre con los nombres de archivos asociados a los productos OPERA, esta cadena `.id` contiene el identificador de un mosaico geográfico MGRS. Podemos extraer ese identificador aplicando manipulación de cadenas con Python al atributo `.id` del gránulo. Se realiza y se almacena el resultado en la variable `tile_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e3bc27",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(granule.id)\n",
    "tile_id = granule.id.split('_')[3]\n",
    "print(f\"{tile_id=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95376e0",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad34781",
   "metadata": {},
   "source": [
    "### Resumiendo los resultados de la búsqueda en un DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb8a0d7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Los detalles de los resultados de la búsqueda son complicados de analizar de esta manera. Se extraen algunos campos concretos de los gránulos obtenidos en un `DataFrame` de Pandas utilizando una función de Python. Definiremos la función aquí y la reutilizaremos en cuadernos posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61693eea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# utility to extract search results into a Pandas DataFrame\n",
    "def search_to_dataframe(search_results):\n",
    "    '''Constructs Pandas DataFrame from PySTAC Earthdata search results.\n",
    "    DataFrame columns are determined from search item properties and assets.'''\n",
    "    # Extract granules into a list of searh items\n",
    "    granules = list(search_results.items())\n",
    "    assert granules, \"Error: empty list of search results\"\n",
    "    # Determine column labels from unique properties from all granules\n",
    "    properties = sorted(list({prop for g in granules for prop in g.properties.keys()}))\n",
    "    # Assemble blocks of rows from each granule\n",
    "    blocks = []\n",
    "    for g in granules:\n",
    "        # Leftmost columns determined from properties\n",
    "        left = pd.Series(index=properties)\n",
    "        for p in properties:\n",
    "            left.loc[p] = g.properties.get(p, None)\n",
    "        tile_id = g.id.split('_')[3]\n",
    "        left.loc['tile_id'] = tile_id\n",
    "        left = pd.DataFrame(left).T\n",
    "        right = []\n",
    "        for a in sorted(g.assets.keys()):\n",
    "            href = g.assets[a].href\n",
    "            # Ignore hrefs using Amazon s3 (not currently working with rasterio)\n",
    "            if href.startswith('s3://'):\n",
    "                continue\n",
    "            right.append(pd.DataFrame(data=dict(asset=a, href=href), index=[0]))\n",
    "        # Use outer join to create block from left row and right block\n",
    "        blocks.append(left.join(pd.concat(right, axis=0, ignore_index=True), how='outer'))\n",
    "    # Stack blocks into final dataframe, forward-filling as needed\n",
    "    df = pd.concat(blocks, axis=0, ignore_index=True).ffill(axis=0)\n",
    "    assert len(df), \"Empty DataFrame\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378e1705",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Invocar `search_to_dataframe` en `search_results` codifica la mayor parte de la información importante de la búsqueda como un Pandas `DataFrame`, tal como se muestra a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7984f5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = search_to_dataframe(search_results)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be60ec9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "El método `DataFrame.info` nos permite examinar la estructura de este DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce3abe5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45033008",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Se limpia el DataFrame que contiene los resultados de búsqueda. Esto podría estar incluido en una función, pero vale la pena saber cómo se hace esto con Pandas de manera interactiva.\n",
    "\n",
    "En primer lugar, para estos resultados, solo es necesaria una columna `Datetime`, se pueden eliminar las demás."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14b57f5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(['start_datetime', 'end_datetime'], axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd965244",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "A continuación, se arregla el esquema del `DataFrame` `df` convirtiendo las columnas en tipos de datos sensibles. También será conveniente utilizar la marca de tiempo de la adquisición como índice del DataFrame. Se realiza utilizando el método `DataFrame.set_index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895478f1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df['datetime'] = pd.DatetimeIndex(df['datetime'])\n",
    "df['eo:cloud_cover'] = df['eo:cloud_cover'].astype(np.float16)\n",
    "str_cols = ['asset', 'href', 'tile_id']\n",
    "for col in str_cols:\n",
    "    df[col] = df[col].astype(pd.StringDtype())\n",
    "df = df.set_index('datetime').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efadc516",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc6cf3d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Como resultado se obtiene un DataFrame con un esquema conciso que se puede utilizar para manipulaciones posteriores. Agrupar los resultados de la búsqueda STAC en un `DataFrame` de Pandas de forma razonable es un poco complicado. Varias de las manipulaciones anteriores podrían haberse incluido en la función `search_to_dataframe`. Pero, dado que los resultados de búsqueda de la API de STAC aún están evolucionando, actualmente es mejor ser flexible y utilizar Pandas de forma interactiva para trabajar con los resultados de búsqueda. Se verá esto con más detalle en ejemplos posteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cb0044",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbae6626",
   "metadata": {},
   "source": [
    "## Explorar y refinar los resultados de búsqueda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2191edd3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Si se examina la columna numérica `eo:cloud_cover` del DataFrame `df`, se pueden recopilar estadísticas utilizando agregaciones estándar y el método `DataFrame.agg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a17185",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df['eo:cloud_cover'].agg(['min','mean','median','max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196b2a14",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Observa que hay varias entradas `nan` en esta columna. Las funciones de agregación estadística de Pandas suelen ser \"`nan`-aware\", esto significa que ignoran implícitamente las entradas `nan` al calcular las estadísticas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50042d1",
   "metadata": {},
   "source": [
    "### Filtrado del DataFrame de búsqueda con Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534c2764",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Como primera operación de filtrado, se mantienen solo las filas para las que la cobertura de las nubes es inferior al 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebce9ae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_clear = df.loc[df['eo:cloud_cover']<50]\n",
    "df_clear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577752a0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Para esta consulta de búsqueda, cada gránulo DSWX comprende datos ráster para diez bandas o niveles. Se puede ver esto aplicando el método Pandas `Series.value_counts` a la columna `asset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c9eecb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_clear.asset.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4c043a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Se filtran las filas que corresponden a la banda `B01_WTR` del producto de datos DSWx. La función de Pandas `DataFrame.str` hace que esta operación sea sencilla. Se nombra al `DataFrame` filtrado como `b01_wtr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21ddc72",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "b01_wtr = df_clear.loc[df_clear.asset.str.contains('B01_WTR')]\n",
    "b01_wtr.info()\n",
    "b01_wtr.asset.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc1af38",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "También se puede observar que hay varios mosaicos geográficos asociados a los gránulos encontrados que intersecan el AOI proporcionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2481c805",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "b01_wtr.tile_id.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b3f477",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Recuerda que estos códigos se refieren a mosaicos geográficos MGRS especificados en un sistema de coordenadas concreto. Como se identifican estos códigos en la columna `tile_id`, se puede filtrar las filas que corresponden, por ejemplo, a los archivos recopilados sobre el mosaico T15RUQ del MGRS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81a8514",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "b01_wtr_t15ruq = b01_wtr.loc[b01_wtr.tile_id=='T15RUQ']\n",
    "b01_wtr_t15ruq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21400edf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Se obtiene un `DataFrame` `b01_wtr_t15ruq` mucho más corto que resume las ubicaciones remotas de los archivos (por ejemplo, GeoTiffs) que almacenan datos ráster para la banda de aguas superficiales `B01_WTR` en el mosaico MGRS `T15RUQ` recopilados en varias marcas de tiempo que se encuentran dentro de la ventana de tiempo que especificamos. Se puede utilizar este DataFrame para descargar esos archivos para su análisis o visualización."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b03fa7",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aff69d3",
   "metadata": {},
   "source": [
    "## Procesamiento de datos para obtener resultados relevantes\n",
    "\n",
    "### Apilamiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edb2bb8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Se cuenta con un `DataFrame` que identifica archivos remotos específicos de datos ráster. El siguiente paso es combinar estos datos ráster en una estructura de datos adecuada para el análisis. El Xarray `DataArray` es adecuado en este caso. La combinación puede generarse utilizando la función `concat` de Xarray. La función `stack_time_slices` en la siguiente celda es larga pero no es complicada. Toma un `DataFrame` con marcas de tiempo en el índice y una columna etiquetada `href` de las URL, lee los archivos asociados a esas URL uno a uno, y apila las matrices bidimensionales relevantes de datos ráster en una matriz tridimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5af639f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def stack_time_slices(granule_dataframe):\n",
    "    '''This function returns a three-dimensional Xarray DataArray comprising time slices read from GeoTIFF files.\n",
    "    - Input: a DataFrame of granules (i.e., a DataFrame with a DateTimeIndex and a column 'href' of URIs).\n",
    "    - Output: a stacked DataArray with dimensions ('time', 'longitude', 'latitude')\n",
    "    - GeoTIFF data are assumed to have been acquired over the same MGRS tile (NOT verified within).\n",
    "    - Note CRS explicitly embedded into DataArray stack as extracted from GeoTIFF file.\n",
    "    - DataArray is constructed using np.datetime64 time axis to simplify visualization.'''\n",
    "    slices, timestamps = list(), list()\n",
    "    for timestamp_, row_ in granule_dataframe.iterrows():\n",
    "        da_ = rio.open_rasterio(row_['href'])\n",
    "        # Preserve coordinate arrays from last GeoTIFF file parsed\n",
    "        x, y = da_.coords['x'].values, da_.coords['y'].values\n",
    "        slices.append(da_.values)\n",
    "        timestamps.append(np.datetime64(timestamp_,'s'))\n",
    "    # Construct time axis from accumulated timestamps\n",
    "    time = np.array(timestamps)\n",
    "    # Construct DataArray stack from accumulated slices & coordinates\n",
    "    slices = np.concatenate(slices, axis=0)\n",
    "    coords = dict(time=time, longitude=x, latitude=y)\n",
    "    stack = xr.DataArray(data=slices, coords=coords, dims=['time', 'latitude', 'longitude'])\n",
    "    # Preserve coordinate reference system (CRS) in DataArray stack\n",
    "    crs = da_.rio.crs\n",
    "    stack.rio.write_crs(crs, inplace=True)\n",
    "    return stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33892e2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "stack = stack_time_slices(b01_wtr_t15ruq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787482d3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c284875c",
   "metadata": {},
   "source": [
    "### Crear una visualización de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043ec3de",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#  Define a colormap with RGBA tuples\n",
    "COLORS = [(150, 150, 150, 0.1)]*256  # Setting all values to gray with low opacity\n",
    "COLORS[0] = (0, 255, 0, 0.1)         # Not-water class to green\n",
    "COLORS[1] = (0, 0, 255, 1)           # Open surface water\n",
    "COLORS[2] = (0, 0, 255, 1)           # Partial surface water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deff2993",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "image_opts = dict(\n",
    "                   x='longitude',\n",
    "                   y='latitude',\n",
    "                   project=True,\n",
    "                   rasterize=True,\n",
    "                   cmap=COLORS, \n",
    "                   colorbar=False,\n",
    "                   tiles = gv.tile_sources.OSM,\n",
    "                   widget_location='bottom',\n",
    "                   frame_width=500,\n",
    "                   frame_height=500,\n",
    "                   xlabel='Longitude (degrees)',\n",
    "                   ylabel='Latitude (degrees)',\n",
    "                   title = 'DSWx data for May 2024 Texas floods',\n",
    "                   fontscale=1.25\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bef4b3a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Visualizar las imágenes completas puede consumir mucha memoria. Se utiliza el método Xarray `DataArray.isel` para extraer un trozo del arreglo `stack` con menos píxeles. Esto permitirá un rápido renderizado y desplazamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0880b0b4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "view = stack.isel(longitude=slice(3000,None), latitude=slice(3000,None))\n",
    "view.hvplot.image(**image_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b825a21",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "stack.hvplot.image(**image_opts) # Construct view from all slices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498e12cf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Antes de continuar, recuerda apagar el kernel de este cuaderno computacional para liberar memoria para otros cálculos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440896fa",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf032c6c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Este cuaderno computacional proporciona principalmente un ejemplo para ilustrar el uso de la API de PySTAC.\n",
    "\n",
    "En los siguientes cuadernos computacionales, utilizaremos este flujo de trabajo general:\n",
    "\n",
    "1. Establecer una consulta de búsqueda mediante la identificación de un _AOI_ particular y un _intervalo de fechas_.\n",
    "2. Identificar un _endpoint_, un _proveedor_ y un _catálogo de activos_ adecuados, y ejecutar la búsqueda utilizando `pystac.Client`.\n",
    "3. Convertir los resultados de la búsqueda en un DataFrame de Pandas que contenga los principales campos de interés.\n",
    "4. Utilizar el DataFrame resultante para filtrar los archivos de datos remotos más relevantes necesarios para el análisis y/o la visualización.\n",
    "5. Ejecutar el análisis y/o la visualización utilizando el DataFrame para recuperar los datos requeridos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5982e2d7",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
