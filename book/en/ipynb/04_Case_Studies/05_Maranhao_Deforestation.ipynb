{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b4d9acf",
   "metadata": {},
   "source": [
    "# Maranhão Deforestation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca1b7ff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "[Deforestation of the Amazon Rainforest in Brazil](https://www.cfr.org/amazon-deforestation/#/en) is an ongoing challenge; in this notebook, we'll use [OPERA DIST-HLS data product](https://lpdaac.usgs.gov/documents/1766/OPERA_DIST_HLS_Product_Specification_V1.pdf) to study the evolution of vegetation loss due to natural and anthropogenic causes. In particular, we'll examine deforestation over a period of roughly two years in the state of Maranhão, Brazil.\n",
    "\n",
    "<center>\n",
    "   <img src=\"https://www.querencianews.com.br/wp-content/uploads/2023/03/WhatsApp-Image-2023-03-30-at-11.22.47-AM.jpeg\"></img><br>\n",
    "   (from https://www.querencianews.com.br/video-de-drone-mostra-cidade-do-maranhao-que-corre-risco-de-desaparecer-por-causa-de-crateras)\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b58c2c",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046e2163",
   "metadata": {},
   "source": [
    "## Outline of steps for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeb19d0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "+ Identifying search parameters (AOI, time-window, endpoint, etc.)\n",
    "+ Obtaining search results\n",
    "+ Exploring & refining search results\n",
    "+ Data-wrangling to produce relevant output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef2e7a8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "In this case, we'll assemble a DataFrame to summarize search results, trim down the results to a manageable size, and make an interactive slider to examine the data retrieved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef348e25",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a3a3b4",
   "metadata": {},
   "source": [
    "### Preliminary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf4ad30",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "import numpy as np, pandas as pd, xarray as xr\n",
    "import rioxarray as rio\n",
    "import rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a90f451",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import hvplot.pandas, hvplot.xarray\n",
    "import geoviews as gv\n",
    "from geoviews import opts\n",
    "gv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5099ea87",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pystac_client import Client\n",
    "from osgeo import gdal\n",
    "# GDAL setup for accessing cloud data\n",
    "gdal.SetConfigOption('GDAL_HTTP_COOKIEFILE','~/.cookies.txt')\n",
    "gdal.SetConfigOption('GDAL_HTTP_COOKIEJAR', '~/.cookies.txt')\n",
    "gdal.SetConfigOption('GDAL_DISABLE_READDIR_ON_OPEN','EMPTY_DIR')\n",
    "gdal.SetConfigOption('CPL_VSIL_CURL_ALLOWED_EXTENSIONS','TIF, TIFF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf27bd14",
   "metadata": {},
   "source": [
    "### Convenient utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce02cb6c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "These functions could be placed in module files for more developed research projects. For learning purposes, they are embedded within this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81302d2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# simple utility to make a rectangle with given center of width dx & height dy\n",
    "def make_bbox(pt,dx,dy):\n",
    "    '''Returns bounding-box represented as tuple (x_lo, y_lo, x_hi, y_hi)\n",
    "    given inputs pt=(x, y), width & height dx & dy respectively,\n",
    "    where x_lo = x-dx/2, x_hi=x+dx/2, y_lo = y-dy/2, y_hi = y+dy/2.\n",
    "    '''\n",
    "    return tuple(coord+sgn*delta for sgn in (-1,+1) for coord,delta in zip(pt, (dx/2,dy/2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ce28a6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# simple utility to plot an AOI or bounding-box\n",
    "def plot_bbox(bbox):\n",
    "    '''Given bounding-box, returns GeoViews plot of Rectangle & Point at center\n",
    "    + bbox: bounding-box specified as (lon_min, lat_min, lon_max, lat_max)\n",
    "    Assume longitude-latitude coordinates.\n",
    "    '''\n",
    "    # These plot options are fixed but can be over-ridden\n",
    "    point_opts = opts.Points(size=12, alpha=0.25, color='blue')\n",
    "    rect_opts = opts.Rectangles(line_width=0, alpha=0.1, color='red')\n",
    "    lon_lat = (0.5*sum(bbox[::2]), 0.5*sum(bbox[1::2]))\n",
    "    return (gv.Points([lon_lat]) * gv.Rectangles([bbox])).opts(point_opts, rect_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bc5e90",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# utility to extract search results into a Pandas DataFrame\n",
    "def search_to_dataframe(search_results):\n",
    "    '''Constructs Pandas DataFrame from PySTAC Earthdata search results.\n",
    "    DataFrame columns are determined from search item properties and assets.'''\n",
    "    # Extract granules into a list of searh items\n",
    "    granules = list(search_results.items())\n",
    "    assert granules, \"Error: empty list of search results\"\n",
    "    # Determine column labels from unique properties from all granules\n",
    "    properties = sorted(list({prop for g in granules for prop in g.properties.keys()}))\n",
    "    # Assemble blocks of rows from each granule\n",
    "    blocks = []\n",
    "    for g in granules:\n",
    "        # Leftmost columns determined from properties\n",
    "        left = pd.Series(index=properties)\n",
    "        for p in properties:\n",
    "            left.loc[p] = g.properties.get(p, None)\n",
    "        tile_id = g.id.split('_')[3]\n",
    "        left.loc['tile_id'] = tile_id\n",
    "        left = pd.DataFrame(left).T\n",
    "        right = []\n",
    "        for a in sorted(g.assets.keys()):\n",
    "            href = g.assets[a].href\n",
    "            # Ignore hrefs using Amazon s3 (not currently working with rasterio)\n",
    "            if href.startswith('s3://'):\n",
    "                continue\n",
    "            right.append(pd.DataFrame(data=dict(asset=a, href=href), index=[0]))\n",
    "        # Use outer join to create block from left row and right block\n",
    "        blocks.append(left.join(pd.concat(right, axis=0, ignore_index=True), how='outer'))\n",
    "    # Stack blocks into final dataframe, forward-filling as needed\n",
    "    df = pd.concat(blocks, axis=0, ignore_index=True).ffill(axis=0)\n",
    "    assert len(df), \"Empty DataFrame\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5479fc34",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# utility to process DataFrame of search results & return DataArray of stacked raster images\n",
    "def stack_time_slices(granule_dataframe):\n",
    "    '''This function returns a three-dimensional Xarray DataArray comprising time slices read from GeoTIFF files.\n",
    "    - Input: a DataFrame of granules (i.e., a DataFrame with a DateTimeIndex and a column 'href' of URIs).\n",
    "    - Output: a stacked DataArray with dimensions ('time', 'longitude', 'latitude')\n",
    "    - GeoTIFF data are assumed to have been acquired over the same MGRS tile (NOT verified within).\n",
    "    - Note CRS explicitly embedded into DataArray stack as extracted from GeoTIFF file.\n",
    "    - DataArray is constructed using np.datetime64 time axis to simplify visualization.'''\n",
    "    slices, timestamps = list(), list()\n",
    "    for timestamp_, row_ in granule_dataframe.iterrows():\n",
    "        da_ = rio.open_rasterio(row_['href'])\n",
    "        # Preserve coordinate arrays from last GeoTIFF file parsed\n",
    "        x, y = da_.coords['x'].values, da_.coords['y'].values\n",
    "        slices.append(da_.values)\n",
    "        timestamps.append(np.datetime64(timestamp_,'s'))\n",
    "    # Construct time axis from accumulated timestamps\n",
    "    time = np.array(timestamps)\n",
    "    # Construct DataArray stack from accumulated slices & coordinates\n",
    "    slices = np.concatenate(slices, axis=0)\n",
    "    coords = dict(time=time, longitude=x, latitude=y)\n",
    "    stack = xr.DataArray(data=slices, coords=coords, dims=['time', 'latitude', 'longitude'])\n",
    "    # Preserve coordinate reference system (CRS) in DataArray stack\n",
    "    crs = da_.rio.crs\n",
    "    stack.rio.write_crs(crs, inplace=True)\n",
    "    return stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd517fcf",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79141959",
   "metadata": {},
   "source": [
    "## Obtaining search results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb71b85b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "We'll focus on an area of interest centered at the geographic longitude-latitude coordinates $(-43.65,^{\\circ}, -3.00^{\\circ})$ that lies within the state of Maranhão, Brazil. We'll look at as much data as is available from January 2022 until the end of March 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224aa68f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "AOI = make_bbox((-43.65, -3.00), 0.2, 0.2)\n",
    "DATE_RANGE = \"2022-01-01/2024-03-31\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74481fd7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "The plot generated below illustrates the AOI; the Bokeh Zoom tool is useful to examine the box on several length scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ca9ab4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Optionally plot the AOI\n",
    "basemap = gv.tile_sources.OpenTopoMap(padding=0.1, alpha=0.75)\n",
    "plot_bbox(AOI) * basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413ee787",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "search_params = dict(bbox=AOI, datetime=DATE_RANGE)\n",
    "print(search_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4ac67c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "To execute the search, we define the endpoint URI and instantiate a `Client` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1209b8af",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ENDPOINT = 'https://cmr.earthdata.nasa.gov/stac'\n",
    "PROVIDER = 'LPCLOUD'\n",
    "COLLECTIONS = [\"OPERA_L3_DIST-ALERT-HLS_V1_1\"]\n",
    "search_params.update(collections=COLLECTIONS)\n",
    "print(search_params)\n",
    "\n",
    "catalog = Client.open(f'{ENDPOINT}/{PROVIDER}/')\n",
    "search_results = catalog.search(**search_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b41860",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "The search itself is quite fast and yields a few thousand results that can be more easily examined in a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20f811f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df = search_to_dataframe(search_results)\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102d879e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "We clean the `DataFrame` `df` in typical ways that make sense:\n",
    "\n",
    "+ renaming the `eo:cloud_cover` column as `cloud_cover`;\n",
    "+ casting the `cloud_cover` column as floating-point values;\n",
    "+ dropping extraneous `datetime` columns;\n",
    "+ casting the `datetime` column as `DatetimeIndex`;\n",
    "+ setting the `datetime` column as the `Index`; and\n",
    "+ casting the remaining columns as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdf5411",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = df.rename(columns={'eo:cloud_cover':'cloud_cover'})\n",
    "df.cloud_cover = df.cloud_cover.astype(np.float16)\n",
    "df = df.drop(['start_datetime', 'end_datetime'], axis=1)\n",
    "df.datetime = pd.DatetimeIndex(df.datetime)\n",
    "df = df.set_index('datetime').sort_index()\n",
    "for col in 'asset href tile_id'.split():\n",
    "    df[col] = df[col].astype(pd.StringDtype())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f4edd9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce8179b",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df12a80e",
   "metadata": {},
   "source": [
    "## Exploring & refining search results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609475cf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "The particular band of the DIST-ALERT data that interests us is the `VEG-DIST-STATUS` band, so we'll construct a boolean series `c1` that is `True` whenever the string in the `asset` column includes `VEG-DIST-STATUS` as a sub-string. We can also construct a boolean series `c2` to filter out rows for which the `cloud_cover` exceeds 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd1274b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "c1 = df.asset.str.contains('VEG-DIST-STATUS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70a7f91",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "c2 = df.cloud_cover<20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d600d4e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "If we examine the `tile_id` column, we can see that a single MGRS tile contains the AOI we specified. As such, all the data indexed in `df` corresponds to distinct measurements taken from a fixed geographic tile at different times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11709caa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df.tile_id.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e949f6fe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "We can combine the information above to reduce the `DataFrame` to a much shorter sequence of rows. We can also drop the `asset` and `tile_id` columns because they will be the same in every row after filtering. We really only need the `href` column going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f331ada",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = df.loc[c1 & c2].drop(['asset', 'tile_id', 'cloud_cover'], axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c0b52b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "It looks as though there are only 11 rows remaining after filtering the others out. These can be visualized interactively as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98fe8ec",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f14a079",
   "metadata": {},
   "source": [
    "## Data-wrangling to produce relevant output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353d9978",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22311f20",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Each row of the `DataFrame` is associated with a distinct granule (in this context, a GeoTIFF file produced from an observation made at a given timestamp). We'll use a loop to assemble a stacked `DataArray` from the remote files using `xarray.concat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4926fcd1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "stack = stack_time_slices(df)\n",
    "stack.attrs = dict(description=f\"OPERA DIST: VEG-DIST-STATUS\", units=None)\n",
    "stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f42cb10",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "As a reminder, for the `VEG-DIST-STATUS` band, we interpret the raster values as follows:\n",
    "\n",
    "+ **0:** No disturbance\n",
    "+ **1:** First detection of disturbance with vegetation cover change <50%\n",
    "+ **2:** Provisional detection of disturbance with vegetation cover change <50%\n",
    "+ **3:** Confirmed detection of disturbance with vegetation cover change <50%\n",
    "+ **4:** First detection of disturbance with vegetation cover change ≥50%\n",
    "+ **5:** Provisional detection of disturbance with vegetation cover change ≥50%\n",
    "+ **6:** Confirmed detection of disturbance with vegetation cover change ≥50%\n",
    "+ **7:** Finished detection of disturbance with vegetation cover change <50%\n",
    "+ **8:** Finished detection of disturbance with vegetation cover change ≥50%\n",
    "+ **255** Missing data\n",
    "\n",
    "By applying `np.unique` to the stack of rasters, we see that all these 10 distinct values occur somewhere in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398c3440",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "np.unique(stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29688ebd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "We'll treat the pixels with missing values (i.e., value `255`) the same as pixels with no disturbance (i.e., value `0`). We could assign the value `nan`, but that converts the data to `float32` or `float64` and hence increases the amount of memory required. That is, reassigning `255->0` allows us to ignore the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a4aa5b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "stack = stack.where(stack!=255, other=0)\n",
    "\n",
    "np.unique(stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52341fbd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "We'll define a colormap to identify pixels showing signs of disturbance. Rather than assigning different colors to each of the 8 categories, we'll use [RGBA (\"red green blue alpha\")](https://en.wikipedia.org/wiki/RGBA_color_model) values to assign colors with a transparency value. With the colormap defined in the next cell, most of the pixels will be fully transparent. The remaining pixels are red with strictly positive `alpha` values. The values we really want to see are `3`, `6`, `7`, & `8` (indicating confirmed ongoing disturbance or disturbance that has finished)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27fb5d8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define a colormap using RGBA values; these need to be written manually here...\n",
    "COLORS = [\n",
    "            (255, 255, 255, 0.0),   # No disturbance\n",
    "            (255,   0,   0, 0.25),  # <50% disturbance, first detection\n",
    "            (255,   0,   0, 0.25),  # <50% disturbance, provisional\n",
    "            (255,   0,   0, 0.50),  # <50% disturbance, confirmed, ongoing\n",
    "            (255,   0,   0, 0.50),  # ≥50% disturbance, first detection\n",
    "            (255,   0,   0, 0.50),  # ≥50% disturbance, provisional\n",
    "            (255,   0,   0, 1.00),  # ≥50% disturbance, confirmed, ongoing\n",
    "            (255,   0,   0, 0.75),  # <50% disturbance, finished\n",
    "            (255,   0,   0, 1.00),  # ≥50% disturbance, finished\n",
    "         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ebc9aa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Finally, we're ready to produce visualizations using the array `stack`. \n",
    "\n",
    "+ We define `view` as a subset of `stack` that uses skips `steps` pixels in each direction to speed rendering (change to `steps=1` or `steps=None` when ready to plot at full resolution).\n",
    "+ We define dictionaries `image_opts` and `layout_opts` to control arguments to pass to `hvplot.image`.\n",
    "+ The result, when plotted, is an interactive plot with a slider that allows us to view specific time slices of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef9c68e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "image_opts = dict(\n",
    "                    x='longitude',\n",
    "                    y='latitude',\n",
    "                    cmap=COLORS,\n",
    "                    colorbar=False,\n",
    "                    clim=(-0.5,8.5),\n",
    "                    crs = stack.rio.crs,\n",
    "                    tiles=gv.tile_sources.ESRI,\n",
    "                    tiles_opts=dict(alpha=0.25, padding=0.1),\n",
    "                    project=True,\n",
    "                    rasterize=True,\n",
    "                    widget_location='bottom',\n",
    "                 )\n",
    "\n",
    "layout_opts = dict(\n",
    "                    title = 'Maranhão \\nDisturbance Alerts',\n",
    "                    xlabel='Longitude (°)',ylabel='Latitude (°)',\n",
    "                    fontscale=1.25,\n",
    "                    frame_width=500,\n",
    "                    frame_height=500,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e27938",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "steps = 100\n",
    "subset=slice(0,None,steps)\n",
    "view = stack.isel(longitude=subset, latitude=subset)\n",
    "view.hvplot.image(**image_opts, **layout_opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2d581b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "The slider here allows us to see a trend of increasing deforestation over the course of two years. The earlier rasters have red pixels sparsely distributed over the region, whereas the later rasters have far more red pixels (indicating damaged vegetation). It is a simple matter to use the array `stack` to count pixels in each category and to assemble quantitative measures of deforestation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993da288",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
