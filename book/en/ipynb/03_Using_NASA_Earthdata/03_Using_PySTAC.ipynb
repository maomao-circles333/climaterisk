{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4dc6930",
   "metadata": {},
   "source": [
    "# Using the PySTAC API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8f13b4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "There is an abundance of data searchable through NASA's [Earthdata Search website](https://search.earthdata.nasa.gov). The preceding link connects to a GUI for searching [SpatioTemporal Asset Catalogs (STACs)](https://stacspec.org/) by specifying an *Area of Interest (AOI)* and a *time-window* or *range of dates*.\n",
    "\n",
    "For the sake of reproducibility, we want to be able to search asset catalogs programmatically. This is where the [PySTAC](https://pystac.readthedocs.io/en/stable/) library comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82c2469",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f9b3bc",
   "metadata": {},
   "source": [
    "## Outline of steps for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2efc998",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "+ Identifying search parameters\n",
    "    + AOI, time-window\n",
    "    + Endpoint, Provider, catalog identifier (\"short name\")\n",
    "+ Obtaining search results\n",
    "    + Instrospect, examine to identify features, bands of interest\n",
    "    + Wrap results into a DataFrame for easier exploration\n",
    "+ Exploring & refining search results\n",
    "    + Identify granules of highest value\n",
    "    + Filter extraneous granules with minimal contribution\n",
    "    + Assemble relevant filtered granules into DataFrame\n",
    "    + Identify kind of output to generate\n",
    "+ Data-wrangling to produce relevant output\n",
    "    + Download relevant granules into Xarray DataArray, stacked appropriately\n",
    "    + Do intermediate computations as necessary\n",
    "    + Assemble relevant data slices into visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2538e53b",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4d25fd",
   "metadata": {},
   "source": [
    "## Identifying search parameters\n",
    "\n",
    "### Defining AOI & range of dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d544d9d5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "We'll start by considering a particular example. [Heavy rains severely impacted Southeast Texas in May 2024](https://www.texastribune.org/2024/05/03/texas-floods-weather-harris-county/), resulting in [flooding and causing significant damage to property and human life](https://www.texastribune.org/series/east-texas-floods-2024/).\n",
    " \n",
    "As usual, certain relevant imports are required. The first two cells are familiar (related to data analysis & visualization tools examined already). The third cell includes imports from the `pystac_client` library and `gdal` library followed by some settings required for using [GDAL (the Geospatial Data Abstraction Library)](https://gdal.org). These configuration details enable your notbook sessions to interact with remote sources of geospatial data smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b1c7b3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "# data wrangling imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import rioxarray as rio\n",
    "import rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cee2adf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Imports for plotting\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray\n",
    "import geoviews as gv\n",
    "from geoviews import opts\n",
    "gv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ea67b3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# STAC imports to retrieve cloud data\n",
    "from pystac_client import Client\n",
    "from osgeo import gdal\n",
    "# GDAL setup for accessing cloud data\n",
    "gdal.SetConfigOption('GDAL_HTTP_COOKIEFILE','~/.cookies.txt')\n",
    "gdal.SetConfigOption('GDAL_HTTP_COOKIEJAR', '~/.cookies.txt')\n",
    "gdal.SetConfigOption('GDAL_DISABLE_READDIR_ON_OPEN','EMPTY_DIR')\n",
    "gdal.SetConfigOption('CPL_VSIL_CURL_ALLOWED_EXTENSIONS','TIF, TIFF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518d5abf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Next, let's define geographic search parameters so we can retrieve data pertinent to that flooding event. This involves specifying an *area of interest (AOI)* and a *range of dates*.\n",
    "\n",
    "+ The AOI is specified as a rectangle of longitude-latitude coordinates in a single 4-tuple of the form\n",
    "  $$({\\mathtt{longitude}}_{\\mathrm{min}},{\\mathtt{latitude}}_{\\mathrm{min}},{\\mathtt{longitude}}_{\\mathrm{max}},{\\mathtt{latitude}}_{\\mathrm{max}}),$$\n",
    "  i.e., the lower,left corner coordinates followed by the upper, right corner coordinates.\n",
    "+ The range of dates is specified as a string of the form\n",
    "  $$ {\\mathtt{date}_{\\mathrm{start}}}/{\\mathtt{date}_{\\mathrm{end}}}, $$\n",
    "  where dates are specified in standard ISO 8601 format `YYYY-MM-DD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b26dd07",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Center of the AOI\n",
    "livingston_tx_lonlat = (-95.09,30.69) # (lon, lat) form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8b2602",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "We'll write a few short functions to encapsulate the logic of our generic workflows. For research code, these would be placed in Python module files. For convenience, we'll embed the functions in this notebook and others so they can execute correctly with minimal dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf114aa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# simple utility to make a rectangle with given center of width dx & height dy\n",
    "def make_bbox(pt,dx,dy):\n",
    "    '''Returns bounding-box represented as tuple (x_lo, y_lo, x_hi, y_hi)\n",
    "    given inputs pt=(x, y), width & height dx & dy respectively,\n",
    "    where x_lo = x-dx/2, x_hi=x+dx/2, y_lo = y-dy/2, y_hi = y+dy/2.\n",
    "    '''\n",
    "    return tuple(coord+sgn*delta for sgn in (-1,+1) for coord,delta in zip(pt, (dx/2,dy/2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d3dc5e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# simple utility to plot an AOI or bounding-box\n",
    "def plot_bbox(bbox):\n",
    "    '''Given bounding-box, returns GeoViews plot of Rectangle & Point at center\n",
    "    + bbox: bounding-box specified as (lon_min, lat_min, lon_max, lat_max)\n",
    "    Assume longitude-latitude coordinates.\n",
    "    '''\n",
    "    # These plot options are fixed but can be over-ridden\n",
    "    point_opts = opts.Points(size=12, alpha=0.25, color='blue')\n",
    "    rect_opts = opts.Rectangles(line_width=0, alpha=0.1, color='red')\n",
    "    lon_lat = (0.5*sum(bbox[::2]), 0.5*sum(bbox[1::2]))\n",
    "    return (gv.Points([lon_lat]) * gv.Rectangles([bbox])).opts(point_opts, rect_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545df62f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "AOI = make_bbox(livingston_tx_lonlat, 0.5, 0.25)\n",
    "basemap = gv.tile_sources.OSM.opts(width=500, height=500)\n",
    "plot_bbox(AOI) * basemap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910d0000",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Let's add a date range. The flooding happened primarily between April 30th & May 2nd; we'll set a longer time window covering the months of April & May."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368e5ecd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "start_date, stop_date = '2024-04-01', '2024-05-31'\n",
    "DATE_RANGE = f'{start_date}/{stop_date}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe44f00",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Finally, let's create a dictionary `search_params` that stores the AOI and the range of dates. This dictionary will be used to search for data in STACs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334419b3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "search_params = dict(bbox=AOI, datetime=DATE_RANGE)\n",
    "print(search_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be529e42",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a40750a",
   "metadata": {},
   "source": [
    "## Obtaining search results\n",
    "\n",
    "### Executing a search with the PySTAC API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0734508a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Three other pieces of information are required to initiate a search for data: the *Endpoint* (a URL), the *Provider* (a string representing a path extending the Endpoint), & the *Collection identifiers* (a list of strings referring to specific catalogs). We generally need to experiment with NASA's [Earthdata Search website](https://search.earthdata.nasa.gov) to determine these values correctly for the specific data products we want to retrieve. The [NASA CMR STAC GitHub repository also monitors issues](https://github.com/nasa/cmr-stac/issues) related to the API for Earthdata Cloud search queries.\n",
    "\n",
    "For the search for DSWx data products that we want to execute, these parameters are as defined in the next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739182bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ENDPOINT = 'https://cmr.earthdata.nasa.gov/stac' # base URL for the STAC to search\n",
    "PROVIDER = 'POCLOUD'\n",
    "COLLECTIONS = [\"OPERA_L3_DSWX-HLS_V1_1.0\"]\n",
    "# Update the dictionary opts with list of collections to search\n",
    "search_params.update(collections=COLLECTIONS)\n",
    "print(search_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533d42e8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Having defined the search parameters in the Python dictionary `search_params`, we can instantiate a `Client` and search the spatio-temporal asset catalog using the `Client.search` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61371697",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "catalog = Client.open(f'{ENDPOINT}/{PROVIDER}/')\n",
    "search_results = catalog.search(**search_params)\n",
    "print(f'{type(search_results)=}\\n',search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615f50e5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "The object `search_results` returned by calling the `search` method is of type `ItemSearch`. To retrieve the results, we invoke the `items` method and cast the result as a Python `list` we'll bind to the identifier `granules`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa88d7a0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "granules = list(search_results.items())\n",
    "print(f\"Number of granules found with tiles overlapping given AOI: {len(granules)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8049ffaa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Let's examine the contents of the list `granules`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3d41db",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "granule = granules[0]\n",
    "print(f'{type(granule)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2265f18c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "granule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85354c70",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "The object `granule` has a rich output representation in this Jupyter notebook. We can expand the attributes in the output cell by clicking the triangles.\n",
    "\n",
    "![](../../../assets/img/granule_output_repr.png)\n",
    "\n",
    "The term *granule* refers to a collection of data files (raster data in this case) all associated with raw data acquired by a particular satellite at a fixed timestamp over a particular geographic tile. There are a number of interesting attributes associated with this granule.\n",
    "\n",
    "+ `properties['datetime']`: a string representing the time of data acquisition for the raster data files in this granule;\n",
    "+ `properties['eo:cloud_cover']`: the percentage of pixels obscured by cloud and cloud shadow in this granule's raster data files; and\n",
    "+ `assets`: a Python `dict` whose values summarize the bands or levels of raster data associated with this granule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2b9cfb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(f\"{type(granule.properties)=}\\n\")\n",
    "print(f\"{granule.properties['datetime']=}\\n\")\n",
    "print(f\"{granule.properties['eo:cloud_cover']=}\\n\")\n",
    "print(f\"{type(granule.assets)=}\\n\")\n",
    "print(f\"{granule.assets.keys()=}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f2b34c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Each object in `granule.assets` is an instance of the `Asset` class that has an attribute `href`. It is the `href` attribute that tells us where to locate a GeoTiff file associated with this asset of this granule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f1411d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for a in granule.assets:\n",
    "    print(f\"{a=}\\t{type(granule.assets[a])=}\")\n",
    "    print(f\"{granule.assets[a].href=}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28619ef0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "In addition, the `Item` has an `.id` attribute that stores a string. As with the filenames associated with OPERA products, this `.id` string contains the identifier for an MGRS geographic tile. We can extract that identifier applying Python string manipulations to the granule `.id` attribute. Let's do that and store the result in `tile_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c1364e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(granule.id)\n",
    "tile_id = granule.id.split('_')[3]\n",
    "print(f\"{tile_id=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9a58cc",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83447bb7",
   "metadata": {},
   "source": [
    "### Summarizing search results in a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310b1aca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "The details of the search results are complicated to parse in this manner. Let's extract a few particular fields from the granules obtained into a Pandas `DataFrame` using a convenient Python function. We'll define the function here and re-use it in later notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87872e5b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# utility to extract search results into a Pandas DataFrame\n",
    "def search_to_dataframe(search_results):\n",
    "    '''Constructs Pandas DataFrame from PySTAC Earthdata search results.\n",
    "    DataFrame columns are determined from search item properties and assets.'''\n",
    "    # Extract granules into a list of searh items\n",
    "    granules = list(search_results.items())\n",
    "    assert granules, \"Error: empty list of search results\"\n",
    "    # Determine column labels from unique properties from all granules\n",
    "    properties = sorted(list({prop for g in granules for prop in g.properties.keys()}))\n",
    "    # Assemble blocks of rows from each granule\n",
    "    blocks = []\n",
    "    for g in granules:\n",
    "        # Leftmost columns determined from properties\n",
    "        left = pd.Series(index=properties)\n",
    "        for p in properties:\n",
    "            left.loc[p] = g.properties.get(p, None)\n",
    "        tile_id = g.id.split('_')[3]\n",
    "        left.loc['tile_id'] = tile_id\n",
    "        left = pd.DataFrame(left).T\n",
    "        right = []\n",
    "        for a in sorted(g.assets.keys()):\n",
    "            href = g.assets[a].href\n",
    "            # Ignore hrefs using Amazon s3 (not currently working with rasterio)\n",
    "            if href.startswith('s3://'):\n",
    "                continue\n",
    "            right.append(pd.DataFrame(data=dict(asset=a, href=href), index=[0]))\n",
    "        # Use outer join to create block from left row and right block\n",
    "        blocks.append(left.join(pd.concat(right, axis=0, ignore_index=True), how='outer'))\n",
    "    # Stack blocks into final dataframe, forward-filling as needed\n",
    "    df = pd.concat(blocks, axis=0, ignore_index=True).ffill(axis=0)\n",
    "    assert len(df), \"Empty DataFrame\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad160500",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Invoking `search_to_dataframe` on `search_results` encodes most of the important information from the search as a Pandas `DataFrame` as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde501d9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = search_to_dataframe(search_results)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e21c39",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "The `DataFrame.info` method allows us to examine the schema of this DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3c0e4f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b728068",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Let's clean up the DataFrame of search results. This could be embedded in a function, but, it's worth knowing how to do this interactiely with Pandas. \n",
    "\n",
    "First, for these results, only one `Datetime` column is necessary; we can drop the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebb483f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(['start_datetime', 'end_datetime'], axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903c6340",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Next, let's fix the schema of the `DataFrame` `df` by casting the columns as sensible data types. It will also be convenient to use the acquisition timestamp as the DataFrame index. Let's do so using the `DataFrame.set_index` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0414a451",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df['datetime'] = pd.DatetimeIndex(df['datetime'])\n",
    "df['eo:cloud_cover'] = df['eo:cloud_cover'].astype(np.float16)\n",
    "str_cols = ['asset', 'href', 'tile_id']\n",
    "for col in str_cols:\n",
    "    df[col] = df[col].astype(pd.StringDtype())\n",
    "df = df.set_index('datetime').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9323a763",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad54c9f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "This finally gives a DataFrame with a concise schema that can be used for later manipulations. Bundling the STAC search results into a Pandas `DataFrame` sensibly is a bit tricky. A number of the manipulations above could have been embedded within the function `search_to_dataframe`. But, given that the STAC API search results are still evolving, it's currently better to be flexible and to use Pandas interactively to work with search results. We'll see this more in later examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23c36c9",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fbe59f",
   "metadata": {},
   "source": [
    "## Exploring & refining search results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f154f54a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "If we examine the numerical `eo:cloud_cover` column of the DataFrame `df`, we can gather statistics using standard aggregations and the `DataFrame.agg` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44cc2f3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df['eo:cloud_cover'].agg(['min','mean','median','max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff4a947",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Notice that there are a number of `nan` entries in this column; Pandads statistical aggregation functions are typically \"`nan`-aware\" meaning that they implicitly ignore `nan` entries (\"Not-a-Number\") when computing statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457a1a91",
   "metadata": {},
   "source": [
    "### Filtering the search DataFrame with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1667d91",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "As a first filtering operation, let's keep only the rows for which the cloud cover is less than 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175c81bd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_clear = df.loc[df['eo:cloud_cover']<50]\n",
    "df_clear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68d529b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "For this search query, each DSWX granule comprises raster data for ten bands or levels. We can see this by applying the Pandas `Series.value_counts` method to the `asset` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac079b22",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_clear.asset.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8249a2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Let's filter out the rows that correspond to the band `B01_WTR` of the DSWx data product. The Pandas `DataFrame.str` accessor makes this operation simple. We'll call the filtered `DataFrame` `b01_wtr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651beb2a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "b01_wtr = df_clear.loc[df_clear.asset.str.contains('B01_WTR')]\n",
    "b01_wtr.info()\n",
    "b01_wtr.asset.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0697b7d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "We can also see that there are several geographic tiles associated with the granules found that intersect the AOI provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd615c04",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "b01_wtr.tile_id.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8df364e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Remember, these codes refer to MGRS geographic tiles specified in a particular coordinate system. As we have identified these codes in the `tile_id` column, we can filter rows that correspond to, say, files collected over the MGRS tile `T15RUQ`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6634f168",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "b01_wtr_t15ruq = b01_wtr.loc[b01_wtr.tile_id=='T15RUQ']\n",
    "b01_wtr_t15ruq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90bb8d0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "We now have a much shorter `DataFrame` `b01_wtr_t15ruq` that summarises the remote locations of files (i.e., GeoTiffs) that store raster data for the surface water band `B01_WTR` in MGRS tile `T15RUQ` collected at various time-stamps that lie within the time-window we specified. We canuse this DataFrame to download those files for analysis or visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fdd5b1",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26322876",
   "metadata": {},
   "source": [
    "## Data-wrangling to produce relevant output\n",
    "\n",
    "### Stacking the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb24c8a1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "We have a `DataFrame` that identifies specific remote files of raster data. The next step is to combine this raster data into a data structure suitable for analysis. The Xarray `DataArray` is suitable in this case; the combination can be generated using the Xarray function `concat`. The function `stack_time_slices` in the next cell is long but not complicated; it takes a `DataFrame` with timestamps on the index and a column labelled `href` of URLs, it reads the files associated with those URLs one-by-one, and it stacks the relevant two-dimensional arrays of raster data into a three-dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d417a2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def stack_time_slices(granule_dataframe):\n",
    "    '''This function returns a three-dimensional Xarray DataArray comprising time slices read from GeoTIFF files.\n",
    "    - Input: a DataFrame of granules (i.e., a DataFrame with a DateTimeIndex and a column 'href' of URIs).\n",
    "    - Output: a stacked DataArray with dimensions ('time', 'longitude', 'latitude')\n",
    "    - GeoTIFF data are assumed to have been acquired over the same MGRS tile (NOT verified within).\n",
    "    - Note CRS explicitly embedded into DataArray stack as extracted from GeoTIFF file.\n",
    "    - DataArray is constructed using np.datetime64 time axis to simplify visualization.'''\n",
    "    slices, timestamps = list(), list()\n",
    "    for timestamp_, row_ in granule_dataframe.iterrows():\n",
    "        da_ = rio.open_rasterio(row_['href'])\n",
    "        # Preserve coordinate arrays from last GeoTIFF file parsed\n",
    "        x, y = da_.coords['x'].values, da_.coords['y'].values\n",
    "        slices.append(da_.values)\n",
    "        timestamps.append(np.datetime64(timestamp_,'s'))\n",
    "    # Construct time axis from accumulated timestamps\n",
    "    time = np.array(timestamps)\n",
    "    # Construct DataArray stack from accumulated slices & coordinates\n",
    "    slices = np.concatenate(slices, axis=0)\n",
    "    coords = dict(time=time, longitude=x, latitude=y)\n",
    "    stack = xr.DataArray(data=slices, coords=coords, dims=['time', 'latitude', 'longitude'])\n",
    "    # Preserve coordinate reference system (CRS) in DataArray stack\n",
    "    crs = da_.rio.crs\n",
    "    stack.rio.write_crs(crs, inplace=True)\n",
    "    return stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e0fc29",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "stack = stack_time_slices(b01_wtr_t15ruq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7884c32c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372300f2",
   "metadata": {},
   "source": [
    "### Producing a data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a5456d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#  Define a colormap with RGBA tuples\n",
    "COLORS = [(150, 150, 150, 0.1)]*256  # Setting all values to gray with low opacity\n",
    "COLORS[0] = (0, 255, 0, 0.1)         # Not-water class to green\n",
    "COLORS[1] = (0, 0, 255, 1)           # Open surface water\n",
    "COLORS[2] = (0, 0, 255, 1)           # Partial surface water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1476f42",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "image_opts = dict(\n",
    "                   x='longitude',\n",
    "                   y='latitude',\n",
    "                   project=True,\n",
    "                   rasterize=True,\n",
    "                   cmap=COLORS, \n",
    "                   colorbar=False,\n",
    "                   tiles = gv.tile_sources.OSM,\n",
    "                   widget_location='bottom',\n",
    "                   frame_width=500,\n",
    "                   frame_height=500,\n",
    "                   xlabel='Longitude (degrees)',\n",
    "                   ylabel='Latitude (degrees)',\n",
    "                   title = 'DSWx data for May 2024 Texas floods',\n",
    "                   fontscale=1.25\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75a7341",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Plotting the images in entirety can use a lot of memory. Let's use the Xarray `DataArray.isel` method to extract a slice from the array `stack` with fewer pixels. This will allow faster rendering and scrolling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6844fafd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "view = stack.isel(longitude=slice(3000,None), latitude=slice(3000,None))\n",
    "view.hvplot.image(**image_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eb1d93",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "stack.hvplot.image(**image_opts) # Construct view from all slices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc83d9ee",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Before continuing, remember to shut down the kernel for this notebook to free up memory for other computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610c7fbe",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc174648",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "This notebook primarily provides an example to illustrate using the PySTAC API.\n",
    "\n",
    "In subsequent notebooks, we'll use this general workflow:\n",
    "\n",
    "1. Set up a search query by identifying a particular *AOI* and *range of dates*.\n",
    "2. Identify a suitable *endpoint*, *provider*, & *asset catalog* and execute the search using `pystac.Client`.\n",
    "3. Convert the search results into a Pandas DataFrame containing the principal fields of interest.\n",
    "4. Use the resulting DataFrame to filter for the most relevant remote data files needed for analysis and/or visualization.\n",
    "5. Execute the analysis and/or visualization using the DataFrame to retrieve the required data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b86fdf",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
